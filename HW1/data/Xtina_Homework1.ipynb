{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from sklearn import preprocessing\n",
    "import os.path\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the data\n",
    "train_df = pd.read_csv('snli_train.tsv', sep=\"\\t\")\n",
    "val_df = pd.read_csv('snli_val.tsv',sep=\"\\t\")\n",
    "\n",
    "#get data & convert sentences to lists\n",
    "train_sentence1=train_df['sentence1'].values.tolist()\n",
    "train_sentence2=train_df['sentence2'].values.tolist()\n",
    "val_sentence1=val_df['sentence1'].values.tolist()\n",
    "val_sentence2=val_df['sentence2'].values.tolist()\n",
    "\n",
    "#convert the text labels to numeric\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "train_targets=le.transform(train_df['label']).tolist()\n",
    "val_targets=le.transform(val_df['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "##Create smaller versions of test for code testing purposes\n",
    "train_sentence1_testing=\n",
    "train_sentence2_testing="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set tokens\n",
    "if not os.path.exists('train_all_combined_sentence_tokens.p'):\n",
    "    print(\"Tokenizing train data\")\n",
    "    train_sentence1_tokens, train_all_sentence1_tokens = tokenize_dataset(train_sentence1)\n",
    "    train_sentence2_tokens, train_all_sentence2_tokens = tokenize_dataset(train_sentence2)\n",
    "    train_all_combined_sentence_tokens = train_all_sentence1_tokens + train_all_sentence2_tokens\n",
    "    pkl.dump(train_all_combined_sentence_tokens, open(\"train_all_combined_sentence_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pickle files do not exist, collect them.  Otherwise, this if block will not run\n",
    "if not os.path.exists('train_sentence1_tokens.p'):\n",
    "    print (\"Tokenizing train data\")\n",
    "    train_sentence1_tokens, train_all_sentence1_tokens = tokenize_dataset(train_sentence1)\n",
    "    train_sentence2_tokens, train_all_sentence2_tokens = tokenize_dataset(train_sentence2)\n",
    "    pkl.dump(train_sentence1_tokens, open(\"train_sentence1_tokens.p\", \"wb\"))\n",
    "    pkl.dump(train_sentence2_tokens, open(\"train_sentence2_tokens.p\", \"wb\"))\n",
    "    #pkl.dump(train_all_sentence1_tokens, open(\"train_all_sentence1_tokens.p\", \"wb\"))\n",
    "    #pkl.dump(train_all_sentence2_tokens, open(\"train_all_sentence2_tokens.p\", \"wb\"))1\n",
    "\n",
    "    #combine tokens from both sentences to create a shared dictionary\n",
    "    train_all_combined_sentence_tokens = train_all_sentence1_tokens + train_all_sentence2_tokens\n",
    "    pkl.dump(train_all_combined_sentence_tokens, open(\"train_all_combined_sentence_tokens.p\", \"wb\"))\n",
    "\n",
    "    #val set tokens\n",
    "    print (\"Tokenizing val data\")\n",
    "    val_sentence1_tokens, _ = tokenize_dataset(val_sentence1)\n",
    "    val_sentence2_tokens, _ = tokenize_dataset(val_sentence2)\n",
    "    pkl.dump(val_sentence1_tokens, open(\"val_sentence1_tokens.p\", \"wb\"))\n",
    "    pkl.dump(val_sentence2_tokens, open(\"val_sentence2_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have previously run the previous cell, run this cell instead to load preprocessed datasets\n",
    "train_sentence1_tokens = pkl.load(open(\"train_sentence1_tokens.p\", \"rb\"))\n",
    "train_sentence2_tokens = pkl.load(open(\"train_sentence2_tokens.p\", \"rb\"))\n",
    "train_all_combined_sentence_tokens = pkl.load(open(\"train_all_combined_sentence_tokens.p\", \"rb\"))\n",
    "train_all_sentence1_tokens = pkl.load(open(\"train_all_sentence1_tokens.p\", \"rb\"))\n",
    "train_all_sentence2_tokens = pkl.load(open(\"train_all_sentence2_tokens.p\", \"rb\"))\n",
    "val_sentence1_tokens = pkl.load(open(\"val_sentence1_tokens.p\", \"rb\"))\n",
    "val_sentence2_tokens = pkl.load(open(\"val_sentence2_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n",
      "\n",
      "Total number of tokens in sentence1 train dataset is 1294135\n",
      "Total number of tokens in sentence2 train dataset is 743372\n",
      "Total number of tokens in combined sent1 & sent2 train dataset is 2037507\n",
      "\n",
      "Total number of *unique* tokens in sentence1 train dataset is 14131\n",
      "Total number of *unique* tokens in sentence2 train dataset is 15225\n",
      "Total number of *unique* tokens in sent1 & sent2 train dataset is 19642\n"
     ]
    }
   ],
   "source": [
    "#print information about the token datasets\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_tokens)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_tokens)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_tokens)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_tokens)))\n",
    "\n",
    "print (\"\\nTotal number of tokens in sentence1 train dataset is {}\".format(len(train_all_sentence1_tokens)))\n",
    "print (\"Total number of tokens in sentence2 train dataset is {}\".format(len(train_all_sentence2_tokens)))\n",
    "print (\"Total number of tokens in combined sent1 & sent2 train dataset is {}\".format(len(train_all_combined_sentence_tokens)))\n",
    "\n",
    "print (\"\\nTotal number of *unique* tokens in sentence1 train dataset is {}\".format(len(set(train_all_sentence1_tokens))))\n",
    "print (\"Total number of *unique* tokens in sentence2 train dataset is {}\".format(len(set(train_all_sentence2_tokens))))\n",
    "print (\"Total number of *unique* tokens in sent1 & sent2 train dataset is {}\".format(len(set(train_all_combined_sentence_tokens))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocabularies for sentence1 and sentence2\n",
    "from collections import Counter\n",
    "\n",
    "#max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#try different vocab sizes\n",
    "vocab1=10000\n",
    "vocab2=15000\n",
    "\n",
    "token2id_combined_sent, id2token_combined_sent = build_vocab(train_all_combined_sentence_tokens,vocab1)\n",
    "token2id_combined_sent_voc2, id2token_combined_sentvoc2 = build_vocab(train_all_combined_sentence_tokens,vocab2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Token id 3625 ; token hauling\n",
      "Token hauling; token id 3625\n"
=======
      "Token id 4280 ; token changing\n",
      "Token changing; token id 4280\n"
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random \n",
    "\n",
    "random_token_id = random.randint(0, len(id2token_combined_sent)-1)\n",
    "random_token = id2token_combined_sent[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_combined_sent[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_combined_sent[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset.  After running this cell we will have converted the word tokens to indices\n",
    "def token2index_dataset(tokens_data,token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "#create train & val for vocab1 size\n",
    "train_sentence1_data_indices = token2index_dataset(train_sentence1_tokens,token2id_combined_sent)\n",
    "train_sentence2_data_indices = token2index_dataset(train_sentence2_tokens,token2id_combined_sent)\n",
    "val_sentence1_data_indices = token2index_dataset(val_sentence1_tokens,token2id_combined_sent)\n",
    "val_sentence2_data_indices = token2index_dataset(val_sentence2_tokens,token2id_combined_sent)\n",
    "\n",
    "#create train & val for vocab2 size\n",
    "train_sentence1_data_indices_voc2 = token2index_dataset(train_sentence1_tokens,token2id_combined_sent_voc2)\n",
    "train_sentence2_data_indices_voc2 = token2index_dataset(train_sentence2_tokens,token2id_combined_sent_voc2)\n",
    "val_sentence1_data_indices_voc2 = token2index_dataset(val_sentence1_tokens,token2id_combined_sent_voc2)\n",
    "val_sentence2_data_indices_voc2 = token2index_dataset(val_sentence2_tokens,token2id_combined_sent_voc2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_data_indices)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_data_indices)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_data_indices)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['a', 'woman', 'jogs', 'on', 'the', 'beach', 'while', 'carrying', 'a', 'water', 'bottle']\n",
      "[2, 12, 2401, 7, 3, 71, 27, 157, 2, 50, 623]\n",
      "['the', 'woman', 'drinks', 'water']\n",
      "[3, 12, 544, 50]\n"
=======
      "['a', 'mixed', 'martial', 'artist', 'performing', 'a', 'sidekick']\n",
      "[2, 3125, 563, 639, 178, 2, 1]\n",
      "['the', 'artist', 'is', 'asleep']\n",
      "[3, 639, 5, 412]\n"
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
     ]
    }
   ],
   "source": [
    "#visualize a random sentence1 and sentence2 paired training example\n",
    "rand_training_example = random.randint(0, len(train_sentence1) - 1)\n",
    "print (train_sentence1_tokens[rand_training_example])\n",
    "print(train_sentence1_data_indices[rand_training_example])\n",
    "\n",
    "print (train_sentence2_tokens[rand_training_example])\n",
    "print(train_sentence2_data_indices[rand_training_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 average is: 12.94135, std dev is: 5.755700667816214, max is: 78, min is: 2\n",
      "sentence2 average is: 7.43372, std dev is: 3.0907033118046123, max is: 38, min is: 1\n"
     ]
    }
   ],
   "source": [
    "#Check average, max, min sentence lengths to determine word padding\n",
    "total_sent1_len=0\n",
    "total_sent2_len=0\n",
    "sent1_lens=[]\n",
    "sent2_lens=[]\n",
    "for i in range(0,len(train_sentence1_tokens)):\n",
    "    total_sent1_len+=len(train_sentence1_tokens[i])\n",
    "    total_sent2_len+=len(train_sentence2_tokens[i])\n",
    "    sent1_lens.append(len(train_sentence1_tokens[i]))\n",
    "    sent2_lens.append(len(train_sentence2_tokens[i]))\n",
    "\n",
    "avg1=total_sent1_len/len(train_sentence1)\n",
    "avg2=total_sent2_len/len(train_sentence2)\n",
    "print(\"sentence1 average is: \"+str(avg1)+\", std dev is: \"+str(np.std(sent1_lens))+\", max is: \"+str(max(sent1_lens))+\", min is: \"+str(min(sent1_lens)))\n",
    "print(\"sentence2 average is: \"+str(avg2)+\", std dev is: \"+str(np.std(sent2_lens))+\", max is: \"+str(max(sent2_lens))+\", min is: \"+str(min(sent2_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list1, data_list2, target_list, MAX_SENTENCE_LENGTH):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.MAX_SENTENCE_LENGTH=MAX_SENTENCE_LENGTH\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.data_list2[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [token_idx1, token_idx2, len(token_idx1), len(token_idx2), label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsGroupDataset(train_sentence1_data_indices,train_sentence2_data_indices, train_targets, 35)\n",
    "val_dataset = NewsGroupDataset(val_sentence1_data_indices, val_sentence2_data_indices, val_targets, 35)\n",
    "\n",
    "train_dataset_voc2 = NewsGroupDataset(train_sentence1_data_indices_voc2,train_sentence2_data_indices_voc2, train_targets, 35)\n",
    "val_dataset_voc2 = NewsGroupDataset(val_sentence1_data_indices_voc2, val_sentence2_data_indices_voc2, val_targets, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH=35\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "    for datum in batch:\n",
    "        \n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2)), torch.LongTensor(length_list1), torch.LongTensor(length_list2), torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "train_loader_voc2 = torch.utils.data.DataLoader(dataset=train_dataset_voc2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader_voc2 = torch.utils.data.DataLoader(dataset=val_dataset_voc2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 150,
=======
   "execution_count": 18,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, comb_method, model_type, pretrained_emb = None):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.frozen_emb=False\n",
    "        \n",
    "        if pretrained_emb is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_emb), freeze=True, padding_idx=0)\n",
    "            self.frozen_emb=True\n",
    "        else:\n",
    "            self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "            \n",
    "        self.model_type=model_type\n",
    "        self.comb_method=comb_method\n",
    "        \n",
    "        #Want 2 hidden layers.  Dims can change out of linear1 must = input of linear2\n",
    "        #if we are using a pretrained embedding, alter the dims to decrease gradually\n",
    "        if self.model_type in ['NN','LG']:\n",
    "            if self.comb_method in ['concat','sum','product']: #=='concat':\n",
    "                if self.comb_method=='concat':\n",
    "                    if self.frozen_emb:\n",
    "                        self.linear1 = nn.Linear(emb_dim*2,300)\n",
    "                    else:\n",
    "                        self.linear1 = nn.Linear(emb_dim*2,50)\n",
    "                else:\n",
    "                    if self.frozen_emb:\n",
    "                        self.linear1 = nn.Linear(emb_dim,300)\n",
    "                    else:\n",
    "                        self.linear1 = nn.Linear(emb_dim,50)\n",
    "                if self.model_type=='NN':\n",
    "                    if self.frozen_emb:\n",
    "                        self.linear2 = nn.Linear(300,150)\n",
    "                        self.linear3 = nn.Linear(150,3)\n",
    "                    else:\n",
    "                        self.linear2 = nn.Linear(50,25)\n",
    "                        self.linear3 = nn.Linear(25,3)\n",
    "            else:\n",
    "                raise Exception('Vect comb methods incl concat, sum, or mult. Comb used was: {}'.format(self.comb_method))\n",
    "        else:\n",
    "            raise Exception('Model types incl NN (neural network) or LG (logistic regression).  Model used was: {}'.format(self.model_type))\n",
    "    \n",
    "    def forward(self, data1, data2, lengths1, lengths2):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        data1_vecrep=self.embed(data1)\n",
    "        data2_vecrep=self.embed(data2)\n",
    "        \n",
    "        out1=torch.sum(data1_vecrep, dim=1)\n",
    "        out2=torch.sum(data2_vecrep, dim=1)\n",
    "        \n",
    "        if not self.frozen_emb:\n",
    "            out1 /= lengths1.view(lengths1.size()[0],1).expand_as(out1).float()\n",
    "            out2 /= lengths2.view(lengths2.size()[0],1).expand_as(out2).float()\n",
    "        else:\n",
    "            out1 /= lengths1.view(lengths1.size()[0],1).expand_as(out1).double()\n",
    "            out2 /= lengths2.view(lengths2.size()[0],1).expand_as(out2).double()\n",
    "        \n",
    "        if self.comb_method=='concat':\n",
    "            out=torch.cat((out1,out2), dim=1, out=None)\n",
    "        elif self.comb_method=='sum':\n",
    "            out=out1+out2\n",
    "        elif self.comb_method=='product':\n",
    "            out=out1*out2\n",
    "        \n",
    "        out = self.linear1(out.float())\n",
    "        \n",
    "        if self.model_type=='NN':\n",
    "            out = F.relu(out)\n",
    "            out = self.linear2(out.float())\n",
    "            out = F .relu(out)\n",
    "            out = self.linear3(out.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our Bag of Words model we haven't applied softmax to the output of linear layer. Why?\n",
    "We use `nn.CrossEntropyLoss()` to train. From pytorch documentation for `nn.CrossEntropyLoss()` ( https://pytorch.org/docs/stable/nn.html ) - this criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. So, this is actually exactly the same as minimizing the log likelihood after applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models with various embedding dimensions for hyperparameter tuning\n",
    "model = BagOfWords(len(id2token_combined_sent), 100,'concat','NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 100])\n",
      "torch.Size([50, 200])\n",
      "torch.Size([50])\n",
      "torch.Size([25, 50])\n",
      "torch.Size([25])\n",
      "torch.Size([3, 25])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 246,
=======
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, data_batch2, length_batch1, length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs,labels)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_type, epochs, sent_comb_method, vocab_size, embed_dim, optimizer, learning_rate, regularization, train_acc, val_acc, train_loss, val_loss]\n",
       "Index: []"
      ]
     },
<<<<<<< HEAD
     "execution_count": 246,
=======
     "execution_count": 28,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pandas DF to store results\n",
    "results_df=pd.DataFrame(columns=['model_type','epochs','sent_comb_method','vocab_size','embed_dim',\n",
    "    'optimizer','learning_rate','regularization','train_acc','val_acc','train_loss','val_loss'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, data_batch2, length_batch1, length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs,labels)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 29,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "now running model: 0\n",
      "Epoch: [1/4], Step: [3125/3125], Validation Acc: 65.6\n"
=======
      "now running model: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c50a990a8481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                             \u001b[0;31m# validate every x iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/choose_a_name/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/choose_a_name/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
     ]
    }
   ],
   "source": [
    "#All hyperparams test in grid search\n",
    "num_epochs = 4 # number epoch to train\n",
    "model_types=['NN','LG']\n",
    "comb_methods=['product','concat']\n",
    "dim_testsizes=[100,200]\n",
    "optimizer_type=['Adam']\n",
    "learning_rates=[.01]\n",
    "l2_reg=[0,.00001]\n",
    "vocab_size=[10000,15000]\n",
    "dataloader_list=[[train_loader,val_loader,vocab1+2],[train_loader_voc2,val_loader_voc2,vocab2+2]]\n",
    "#Only use for best 2 models to generate plots.  If True, per batch stats will be computed to get per epoch avgs.  \n",
    "perepoch_stats=False\n",
    "\n",
    "#determines whether to save acc stats calculated during training, or recompute from loaded model\n",
    "loaded_or_created=''\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#create counter for distributing the computations, create list for the numbered loops we want to run\n",
    "counter=0\n",
    "to_run=[0]#,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]\n",
    "\n",
    "#search for best results\n",
    "for regularization in l2_reg:\n",
    "    for learning_rate in learning_rates:\n",
    "        for opt_type in optimizer_type:\n",
    "            for dimension in dim_testsizes:\n",
    "                for combination_method in comb_methods:\n",
    "                    for model_type in model_types:\n",
    "                        for loader in dataloader_list:\n",
    "                            if counter in to_run:\n",
    "                                print(\"now running model: \"+str(counter))\n",
    "                                model=BagOfWords(loader[2], dimension, combination_method, model_type)\n",
    "                                if opt_type=='Adam':\n",
    "                                    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                elif opt_type=='SGD':\n",
    "                                    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                #save_str=('v2_'+model_type+'_'+combination_method+'_'+str(loader[2])+'_'+str(dimension)+'_'+str(regularization).replace('0.',''))\n",
    "                                save_str='testing'+str(counter)\n",
    "                                #check to see if we have already run this model/epoch/comb method/vocab size/embed combination\n",
    "                                #if we haven't run it yet, run it and then save\n",
    "                                if not os.path.exists(save_str):\n",
    "                                    #epoch_models used to implement early stopping\n",
    "                                    epoch_models=[]\n",
    "                                    epoch_avgvalloss=[]\n",
    "                                    epoch_avgvalacc=[]\n",
    "                                    for epoch in range(num_epochs):\n",
    "                                        #vars to store per epoch stats.  Only used if perepoch_stats is True\n",
    "                                        val_losses=[]\n",
    "                                        val_accs=[]\n",
    "                                        for i, (data1, data2, lengths1, lengths2, labels) in enumerate(loader[0]):\n",
    "                                            model.train()\n",
    "                                            data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "                                            optimizer.zero_grad()\n",
    "                                            outputs = model(data_batch1, data_batch2, length_batch1, length_batch2)\n",
    "                                            loss = criterion(outputs, label_batch)\n",
    "                                            loss.backward()\n",
    "                                            optimizer.step()\n",
    "                                            # validate every x iterations\n",
    "                                            if i > 0 and i % 3124 == 0:\n",
    "                                                # validate\n",
    "                                                validation_accuracy = test_model(loader[1], model)[0]\n",
    "                                                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                                                    epoch+1, num_epochs, i+1, len(loader[0]), validation_accuracy))\n",
    "                                            #if the flag to compute per epoch avg stats exists, compute them\n",
    "                                            if perepoch_stats:\n",
    "                                                valacc, valloss = test_model(loader[1],model)\n",
    "                                                val_losses.append(valloss.item())\n",
    "                                                val_accs.append(valacc)\n",
    "                                                \n",
    "                                        #here we have finished the epoch, so save our results\n",
    "                                        save_dict=BagOfWords(loader[2], dimension, combination_method, model_type)\n",
    "                                        save_dict.load_state_dict(model.state_dict())\n",
    "                                        epoch_models.append([epoch,save_dict,test_model(loader[1], model)[0]])\n",
    "                                        \n",
    "                                        #if flag to compute per epoch avg stats exists we now have the full list, so compute them\n",
    "                                        if perepoch_stats:\n",
    "                                            epoch_avgvalloss.append(np.mean(val_losses))\n",
    "                                            epoch_avgvalacc.append(np.mean(val_accs))\n",
    "                                            print(\"epoch avg val loss was: \"+str(np.mean(val_losses)))\n",
    "                                            print(\"epoch avg val acc was: \"+str(np.mean(val_accs)))\n",
    "                                    \n",
    "                                    #here all epochs have run, so we want to save only the model with the best val accuracy                                    print(len(epoch_models))\n",
    "                                    max_acc=max([i[2] for i in epoch_models])\n",
    "                                    print(\"max value was: \"+str(max_acc))\n",
    "                                    max_indx=[i[2] for i in epoch_models].index(max_acc)\n",
    "                                    print(\"index for max value was: \"+str(max_indx))\n",
    "                                    print(\"accuracy for model to be saved was \"+str(test_model(loader[1], epoch_models[max_indx][1])[0]))\n",
    "                                    model=epoch_models[max_indx][1]\n",
    "                                    #now save the model so we don't have to rerun\n",
    "                                    torch.save(model.state_dict(),save_str)\n",
    "                                #if we have already run this model/epoch/comb method/vocab size/embed combination, load instead\n",
    "                                else:\n",
    "                                    model.load_state_dict(torch.load(save_str))\n",
    "\n",
    "                                #now we want to save results, but only if we don't yet have these results in the table\n",
    "                                if not ((results_df['model_type'] == model_type) & (results_df['epochs'] == num_epochs) & \\\n",
    "                                (results_df['sent_comb_method']==combination_method) & \\\n",
    "                                (results_df['vocab_size']==model.embed.num_embeddings-2) & \\\n",
    "                                (results_df['embed_dim']==model.embed.embedding_dim) & \\\n",
    "                                (results_df['learning_rate']==learning_rate) & (results_df['optimizer']==opt_type) & \\\n",
    "                                (results_df['regularization']==regularization)).any():\n",
    "                                        results_df=results_df.append(pd.Series([model_type,num_epochs,combination_method,model.embed.num_embeddings-2,model.embed.embedding_dim,opt_type,learning_rate,regularization,test_model(loader[0], model)[0],test_model(loader[1], model)[0],test_model(loader[0], model)[1].item(),test_model(loader[1], model)[1].item()],index=results_df.columns),ignore_index=True)\n",
    "                                print(\"skipping counter value: \"+str(counter))\n",
    "                            #increment the counter once we are done\n",
    "                            counter+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      "), 64.3], [1, BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      "), 66.1], [2, BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      "), 64.4], [3, BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      "), 63.8], [4, BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      "), 62.3]]\n",
      "5\n",
      "max value was: 66.1\n",
      "index for max value was: 1\n",
      "1\n",
      "BagOfWords(\n",
      "  (embed): Embedding(10002, 100, padding_idx=0)\n",
      "  (linear1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (linear3): Linear(in_features=25, out_features=3, bias=True)\n",
      ")\n",
      "66.1\n"
     ]
    }
   ],
   "source": [
    "print(tosave)\n",
    "print(len(tosave))\n",
    "max_acc=max([i[2] for i in tosave])\n",
    "print(\"max value was: \"+str(max_acc))\n",
    "max_indx=[i[2] for i in tosave].index(max_acc)\n",
    "print(\"index for max value was: \"+str(max_indx))\n",
    "model=tosave[max_indx][1]\n",
    "\n",
    "print(tosave[max_indx][0])\n",
    "print(tosave[max_indx][1])\n",
    "print(tosave[max_indx][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>92.375</td>\n",
       "      <td>63.9</td>\n",
       "      <td>0.618784</td>\n",
       "      <td>1.013288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type epochs sent_comb_method vocab_size embed_dim optimizer  \\\n",
       "0         NN      4          product      10000       100      Adam   \n",
       "\n",
       "   learning_rate regularization  train_acc  val_acc  train_loss  val_loss  \n",
       "0           0.01              0     92.375     63.9    0.618784  1.013288  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
=======
   "execution_count": 30,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create epoch vs. loss and acc plots for our best NN model\n",
    "\n",
    "#To create NN visual, MUST first run model cell with these parameters which will save the per epoch stats,\n",
    "#which will then be saved locally and be used to create the visualization\n",
    "\n",
    "#params used for this visual\n",
    "#model_types=['NN']\n",
    "#dataloader_list=[[train_loader,val_loader,vocab1+2]]\n",
    "#dim_testsizes=[100]\n",
    "#comb_methods=['concat']\n",
    "#learning_rates = [0.01]\n",
    "#optimizer_type=['Adam']\n",
    "#l2_reg=[.00001]\n",
    "#perepoch_stats=False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#if we don't have the plotting data yet, create it\n",
    "if not os.path.exists('NN_losslist.txt'):\n",
    "    with open('NN_losslist.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % loss for loss in epoch_avgvalloss)\n",
    "if not os.path.exists('NN_acclist.txt'):\n",
    "    with open('NN_acclist.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % loss for loss in epoch_avgvalacc)\n",
    "\n",
    "file1 = open(\"NN_losslist.txt\",\"r\")\n",
    "file1=[float(i) for i in file1.read().split(\"\\n\")[0:20]]\n",
    "file2 = open(\"NN_acclist.txt\",\"r\")\n",
    "file2 = [float(i) for i in file2.read().split(\"\\n\")[0:20]]\n",
    "\n",
    "plt.title(\"Best NN Model: sentence_comb=concat,\\nvocab_size=10000, emb_dim=100, reg=.00001\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Validation Loss\")\n",
    "plt.xlim(0,20)\n",
    "plt.plot(file1)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Best NN Model: sentence_comb=concat,\\nvocab_size=10000, emb_dim=100, reg=.00001\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Validation Accuracy\")\n",
    "plt.xlim(0,20)\n",
    "plt.plot(file2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create epoch vs. loss and acc plots for our best LR model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#params used for this visual\n",
    "#model_types=['LG']\n",
    "#dataloader_list=[[train_loader_voc2,val_loader_voc2,vocab2+2]]\n",
    "#dim_testsizes=[200]\n",
    "#comb_methods=['product']\n",
    "#learning_rates = [0.01]\n",
    "#optimizer_type=['Adam']\n",
    "#l2_reg=[.00001]\n",
    "\n",
    "#if we don't have the plotting data yet, create it\n",
    "if not os.path.exists('LR_losslist.txt'):\n",
    "    with open('LR_losslist.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % loss for loss in epoch_avgvalloss)\n",
    "if not os.path.exists('LR_acclist.txt'):\n",
    "    with open('LR_acclist.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % loss for loss in epoch_avgvalacc)\n",
    "        \n",
    "file3 = open(\"LR_losslist.txt\",\"r\")\n",
    "file3 = [float(i) for i in file3.read().split(\"\\n\")[0:20]]\n",
    "file4 = open(\"LR_acclist.txt\",\"r\")\n",
    "file4 = [float(i) for i in file4.read().split(\"\\n\")[0:20]]\n",
    "\n",
    "plt.title(\"Best LR Model: sentence_comb=product,\\nvocab_size=15000, emb_dim=100, reg=.00001\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Validation Loss\")\n",
    "plt.xlim(0,20)\n",
    "plt.plot(file3)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Best LR Model: sentence_comb=product,\\nvocab_size=15000, emb_dim=100, reg=.00001\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Validation Accuracy\")\n",
    "plt.xlim(0,20)\n",
    "plt.plot(file4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our results to a CSV if we don't already have one\n",
    "if not os.path.exists('results_table'):\n",
    "    results_df.to_csv(\"results_table\")\n",
    "else:\n",
    "    results_df=pd.read_csv(\"results_table\",usecols=[1,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>75.856</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.844871</td>\n",
       "      <td>0.997327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>83.245</td>\n",
       "      <td>68.2</td>\n",
       "      <td>0.728596</td>\n",
       "      <td>0.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>83.376</td>\n",
       "      <td>68.1</td>\n",
       "      <td>0.845691</td>\n",
       "      <td>0.877152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.902</td>\n",
       "      <td>67.6</td>\n",
       "      <td>3.215102</td>\n",
       "      <td>3.322199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>76.151</td>\n",
       "      <td>67.1</td>\n",
       "      <td>0.862027</td>\n",
       "      <td>0.937692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>82.543</td>\n",
       "      <td>66.9</td>\n",
       "      <td>0.816725</td>\n",
       "      <td>1.056434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.222</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.827081</td>\n",
       "      <td>1.016528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>74.578</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.852415</td>\n",
       "      <td>0.933229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>91.102</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.652156</td>\n",
       "      <td>1.009378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.752</td>\n",
       "      <td>66.5</td>\n",
       "      <td>3.334356</td>\n",
       "      <td>3.305007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.626</td>\n",
       "      <td>66.1</td>\n",
       "      <td>3.261423</td>\n",
       "      <td>3.270649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>73.900</td>\n",
       "      <td>65.6</td>\n",
       "      <td>0.894433</td>\n",
       "      <td>0.898248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>97.340</td>\n",
       "      <td>64.8</td>\n",
       "      <td>0.584040</td>\n",
       "      <td>0.655986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.675</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.300332</td>\n",
       "      <td>3.525836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.422</td>\n",
       "      <td>63.1</td>\n",
       "      <td>2.956391</td>\n",
       "      <td>3.323969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>88.543</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.703158</td>\n",
       "      <td>0.873891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.729</td>\n",
       "      <td>62.7</td>\n",
       "      <td>0.575442</td>\n",
       "      <td>0.682155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.295</td>\n",
       "      <td>62.6</td>\n",
       "      <td>2.954807</td>\n",
       "      <td>3.298273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66.783</td>\n",
       "      <td>62.4</td>\n",
       "      <td>3.352863</td>\n",
       "      <td>3.504542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.106</td>\n",
       "      <td>62.2</td>\n",
       "      <td>2.947797</td>\n",
       "      <td>3.384999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66.698</td>\n",
       "      <td>62.2</td>\n",
       "      <td>3.392915</td>\n",
       "      <td>3.320192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>86.818</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.793801</td>\n",
       "      <td>1.024558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.270</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.584388</td>\n",
       "      <td>0.766349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>90.940</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.719942</td>\n",
       "      <td>0.773133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>67.357</td>\n",
       "      <td>61.7</td>\n",
       "      <td>3.388863</td>\n",
       "      <td>3.373365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.963</td>\n",
       "      <td>61.5</td>\n",
       "      <td>0.570284</td>\n",
       "      <td>1.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>67.084</td>\n",
       "      <td>61.5</td>\n",
       "      <td>3.415791</td>\n",
       "      <td>3.449479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>70.297</td>\n",
       "      <td>61.2</td>\n",
       "      <td>3.399898</td>\n",
       "      <td>3.307295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.250</td>\n",
       "      <td>60.4</td>\n",
       "      <td>2.949743</td>\n",
       "      <td>3.276784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>73.753</td>\n",
       "      <td>60.3</td>\n",
       "      <td>3.257515</td>\n",
       "      <td>3.439494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>71.696</td>\n",
       "      <td>59.9</td>\n",
       "      <td>3.377287</td>\n",
       "      <td>3.432222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>72.633</td>\n",
       "      <td>59.8</td>\n",
       "      <td>3.292809</td>\n",
       "      <td>3.358293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type sent_comb_method  vocab_size  embed_dim optimizer  \\\n",
       "20         NN           concat       10000        100      Adam   \n",
       "25         NN          product       15000        200      Adam   \n",
       "24         NN          product       10000        200      Adam   \n",
       "27         LG          product       15000        200      Adam   \n",
       "21         NN           concat       15000        100      Adam   \n",
       "17         NN          product       15000        100      Adam   \n",
       "16         NN          product       10000        100      Adam   \n",
       "28         NN           concat       10000        200      Adam   \n",
       "4          NN           concat       10000        100      Adam   \n",
       "19         LG          product       15000        100      Adam   \n",
       "26         LG          product       10000        200      Adam   \n",
       "29         NN           concat       15000        200      Adam   \n",
       "0          NN          product       10000        100      Adam   \n",
       "18         LG          product       10000        100      Adam   \n",
       "11         LG          product       15000        200      Adam   \n",
       "13         NN           concat       15000        200      Adam   \n",
       "8          NN          product       10000        200      Adam   \n",
       "3          LG          product       15000        100      Adam   \n",
       "23         LG           concat       15000        100      Adam   \n",
       "2          LG          product       10000        100      Adam   \n",
       "22         LG           concat       10000        100      Adam   \n",
       "12         NN           concat       10000        200      Adam   \n",
       "1          NN          product       15000        100      Adam   \n",
       "5          NN           concat       15000        100      Adam   \n",
       "31         LG           concat       15000        200      Adam   \n",
       "9          NN          product       15000        200      Adam   \n",
       "30         LG           concat       10000        200      Adam   \n",
       "14         LG           concat       10000        200      Adam   \n",
       "10         LG          product       10000        200      Adam   \n",
       "7          LG           concat       15000        100      Adam   \n",
       "6          LG           concat       10000        100      Adam   \n",
       "15         LG           concat       15000        200      Adam   \n",
       "\n",
       "    learning_rate  regularization  train_acc  val_acc  train_loss  val_loss  \n",
       "20           0.01         0.00001     75.856     70.0    0.844871  0.997327  \n",
       "25           0.01         0.00001     83.245     68.2    0.728596  0.791601  \n",
       "24           0.01         0.00001     83.376     68.1    0.845691  0.877152  \n",
       "27           0.01         0.00001     81.902     67.6    3.215102  3.322199  \n",
       "21           0.01         0.00001     76.151     67.1    0.862027  0.937692  \n",
       "17           0.01         0.00001     82.543     66.9    0.816725  1.056434  \n",
       "16           0.01         0.00001     81.222     66.7    0.827081  1.016528  \n",
       "28           0.01         0.00001     74.578     66.7    0.852415  0.933229  \n",
       "4            0.01         0.00000     91.102     66.7    0.652156  1.009378  \n",
       "19           0.01         0.00001     81.752     66.5    3.334356  3.305007  \n",
       "26           0.01         0.00001     81.626     66.1    3.261423  3.270649  \n",
       "29           0.01         0.00001     73.900     65.6    0.894433  0.898248  \n",
       "0            0.01         0.00000     97.340     64.8    0.584040  0.655986  \n",
       "18           0.01         0.00001     81.675     64.7    3.300332  3.525836  \n",
       "11           0.01         0.00000     99.422     63.1    2.956391  3.323969  \n",
       "13           0.01         0.00000     88.543     63.0    0.703158  0.873891  \n",
       "8            0.01         0.00000     98.729     62.7    0.575442  0.682155  \n",
       "3            0.01         0.00000     99.295     62.6    2.954807  3.298273  \n",
       "23           0.01         0.00001     66.783     62.4    3.352863  3.504542  \n",
       "2            0.01         0.00000     99.106     62.2    2.947797  3.384999  \n",
       "22           0.01         0.00001     66.698     62.2    3.392915  3.320192  \n",
       "12           0.01         0.00000     86.818     61.9    0.793801  1.024558  \n",
       "1            0.01         0.00000     98.270     61.9    0.584388  0.766349  \n",
       "5            0.01         0.00000     90.940     61.7    0.719942  0.773133  \n",
       "31           0.01         0.00001     67.357     61.7    3.388863  3.373365  \n",
       "9            0.01         0.00000     98.963     61.5    0.570284  1.002020  \n",
       "30           0.01         0.00001     67.084     61.5    3.415791  3.449479  \n",
       "14           0.01         0.00000     70.297     61.2    3.399898  3.307295  \n",
       "10           0.01         0.00000     99.250     60.4    2.949743  3.276784  \n",
       "7            0.01         0.00000     73.753     60.3    3.257515  3.439494  \n",
       "6            0.01         0.00000     71.696     59.9    3.377287  3.432222  \n",
       "15           0.01         0.00000     72.633     59.8    3.292809  3.358293  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=['val_acc'],axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Correctly and Incorrectly Classified Sentences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 87,
=======
   "execution_count": 32,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following three instances were classified incorrectly:\n",
      "#########################################################\n",
<<<<<<< HEAD
      "sentence 1:  this wildly dressed couple are walking past a poster \n",
      "sentence 2:  the couple are normally dressed \n",
=======
      "sentence 1:  some young boys ride bikes in the street \n",
      "sentence 2:  there are girls and boys \n",
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
      "Real Label:  contradiction\n",
      "Predicted Label:  neutral\n",
      "\n",
<<<<<<< HEAD
      "sentence 1:  two women playing field hockey on <unk> \n",
      "sentence 2:  women re <unk> a field hockey field \n",
      "Real Label:  contradiction\n",
      "Predicted Label:  neutral\n",
      "\n",
      "sentence 1:  a cowboy clown wearing a shirt with tax on it red suspenders and a white cowboy hat talks into a <unk> microphone \n",
      "sentence 2:  a cowboy clown in a bowler hat is talking into a <unk> microphone \n",
      "Real Label:  contradiction\n",
      "Predicted Label:  neutral\n",
=======
      "sentence 1:  closeup of an elderly woman walking across a street with her male companion \n",
      "sentence 2:  a man helps a woman cross the street \n",
      "Real Label:  neutral\n",
      "Predicted Label:  entailment\n",
      "\n",
      "sentence 1:  boy jumps in desert while others watch and take photo \n",
      "sentence 2:  a group of people without cameras are watching a boy \n",
      "Real Label:  contradiction\n",
      "Predicted Label:  entailment\n",
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
      "\n",
      "\n",
      "\n",
      "The following three instances were classified correctly:\n",
      "#########################################################\n",
<<<<<<< HEAD
      "sentence 1:  two race car drivers smile on a stage next to a female spokes model wearing a white dress and white and blue boots \n",
      "sentence 2:  two race car drivers are waiting for the award ceremony to end \n",
      "Real Label:  neutral\n",
      "Predicted Label:  neutral\n",
      "\n",
      "sentence 1:  a white dog with long hair jumps to catch a red and green toy \n",
      "sentence 2:  a white dog with long hair is swimming underwater \n",
      "Real Label:  contradiction\n",
      "Predicted Label:  contradiction\n",
      "\n",
      "sentence 1:  four young men sit on the floor close to a television that is showing elmo from sesame street \n",
      "sentence 2:  four males are sitting indoors because it 's raining \n",
      "Real Label:  neutral\n",
      "Predicted Label:  neutral\n",
=======
      "sentence 1:  a man in a blue helmet jumping off of a hill on a dirt bike \n",
      "sentence 2:  the man is a professional athlete \n",
      "Real Label:  neutral\n",
      "Predicted Label:  neutral\n",
      "\n",
      "sentence 1:  an older man sitting on a bench on the sidewalk reading a book \n",
      "sentence 2:  an old guy sits on a bench by a cracked sidewalk reading a red book \n",
      "Real Label:  neutral\n",
      "Predicted Label:  neutral\n",
      "\n",
      "sentence 1:  a man wearing lime green pants and shirt working on a sign \n",
      "sentence 2:  a man wearing pants and shirt working on a sign \n",
      "Real Label:  entailment\n",
      "Predicted Label:  entailment\n",
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaklevs/anaconda3/envs/choose_a_name/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "def inverse_tokenize(sent):\n",
    "    output = ''\n",
    "    for word in sent:\n",
    "        output += id2token_combined_sent[word]\n",
    "        output += ' '\n",
    "    return output\n",
    "\n",
    "def inverse_tokenize_15k(sent):\n",
    "    output = ''\n",
    "    for word in sent:\n",
    "        output += id2token_combined_sentvoc2[word]\n",
    "        output += ' '\n",
    "    return output\n",
    "\n",
    "def six_classified_sentences(model_save_path):\n",
    "    filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = model_save_path.split('_')\n",
    "    model=BagOfWords(int(vocab_size), int(emb_dim), combine_type, model_type)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "    \n",
    "    misclassifieds = []\n",
    "    classifieds = []\n",
    "    \n",
    "    #index = -1\n",
    "    while len(misclassifieds) < 3 or len(classifieds) < 3:\n",
    "        #index += 1\n",
    "        index = np.random.randint(len(val_dataset))\n",
    "        sent1, sent2, len1, len2, label = val_dataset[index]\n",
    "        output = F.softmax(model(torch.tensor([sent1]), torch.tensor([sent2]), torch.tensor([len1]), torch.tensor([len2])))\n",
    "        predicted = output.max(1, keepdim=True)[1]\n",
    "        if int(predicted) != label and len(misclassifieds) < 3:\n",
    "            misclassifieds.append([inverse_tokenize(sent1), inverse_tokenize(sent2), label, int(predicted), index])\n",
    "        elif int(predicted) == label and len(classifieds) < 3:\n",
    "            classifieds.append([inverse_tokenize(sent1), inverse_tokenize(sent2), label, int(predicted), index])\n",
    "    return misclassifieds, classifieds\n",
    "\n",
    "incorrect, correct = six_classified_sentences('v2_NN_concat_10002_100_1e-05')\n",
    "\n",
    "print('The following three instances were classified incorrectly:')\n",
    "print('#########################################################')\n",
    "for instance in incorrect:\n",
    "    sent1, sent2, true_label, pred_label, index = instance\n",
    "    print('sentence 1: ', sent1)\n",
    "    print('sentence 2: ', sent2)\n",
    "    print('Real Label: ', le.inverse_transform([true_label])[0])\n",
    "    print('Predicted Label: ', le.inverse_transform([pred_label])[0])\n",
    "    print('')\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print('The following three instances were classified correctly:')\n",
    "print('#########################################################')\n",
    "for instance in correct:\n",
    "    sent1, sent2, true_label, pred_label, index = instance\n",
    "    print('sentence 1: ', sent1)\n",
    "    print('sentence 2: ', sent2)\n",
    "    print('Real Label: ', le.inverse_transform([true_label])[0])\n",
    "    print('Predicted Label: ', le.inverse_transform([pred_label])[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": 34,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_df = pd.read_csv('mnli_train.tsv', sep=\"\\t\")\n",
    "mnli_val_df = pd.read_csv('mnli_val.tsv',sep=\"\\t\")\n",
    "\n",
    "#Creates a list of genres represented in data\n",
    "all_genres = []\n",
    "for genre in mnli_train_df['genre']:\n",
    "    if not genre in all_genres:\n",
    "        all_genres.append(genre)\n",
    "\n",
    "#similar to tokenize dataset function, but tokenizes with the pasted in vocab dictionary\n",
    "def tokenize_with_vocab(dataset,vocab):\n",
    "    output = []\n",
    "    for sentence in dataset:\n",
    "        tokens = tokenize(sentence)\n",
    "        index_list = [vocab[token] if token in vocab.keys() else 1 for token in tokens]\n",
    "        output.append(index_list)\n",
    "        \n",
    "    return output\n",
    "\n",
    "#creates training datasets for each genre\n",
    "for genre in all_genres:\n",
    "    if not os.path.exists('val_sentence2_' +genre+'.p'):\n",
    "        dataset = mnli_val_df.loc[mnli_val_df['genre'] == genre]\n",
    "        sentence1_dataset = list(dataset['sentence1'])\n",
    "        sentence2_dataset = list(dataset['sentence2'])\n",
    "        val_sentence1 = tokenize_with_vocab(sentence1_dataset, token2id_combined_sent)\n",
    "        val_sentence2 = tokenize_with_vocab(sentence2_dataset, token2id_combined_sent)\n",
    "        pkl.dump(val_sentence1, open(\"val_sentence1_\" + genre +\".p\", \"wb\"))\n",
    "        pkl.dump(val_sentence2, open(\"val_sentence2_\" + genre +\".p\", \"wb\"))\n",
    "            \n",
    "val_sentence1_telephone = pkl.load(open(\"val_sentence1_telephone.p\", \"rb\"))\n",
    "val_sentence2_telephone = pkl.load(open(\"val_sentence2_telephone.p\", \"rb\"))\n",
    "val_sentence1_fiction = pkl.load(open(\"val_sentence1_fiction.p\", \"rb\"))\n",
    "val_sentence2_fiction = pkl.load(open(\"val_sentence2_fiction.p\", \"rb\"))\n",
    "val_sentence1_slate = pkl.load(open(\"val_sentence1_slate.p\", \"rb\"))\n",
    "val_sentence2_slate = pkl.load(open(\"val_sentence2_slate.p\", \"rb\"))\n",
    "val_sentence1_government = pkl.load(open(\"val_sentence1_government.p\", \"rb\"))\n",
    "val_sentence2_government = pkl.load(open(\"val_sentence2_government.p\", \"rb\"))\n",
    "val_sentence1_travel = pkl.load(open(\"val_sentence1_travel.p\", \"rb\"))\n",
    "val_sentence2_travel = pkl.load(open(\"val_sentence2_travel.p\", \"rb\"))\n",
    "\n",
    "#intializes lists for the label\n",
    "telephone_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'telephone']['label']))\n",
    "fiction_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'fiction']['label']))\n",
    "slate_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'slate']['label']))\n",
    "government_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'government']['label']))\n",
    "travel_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'travel']['label']))\n",
    "\n",
    "\n",
    "#sets up loaders to pass into test_model function\n",
    "val_dataset_telephone = NewsGroupDataset(val_sentence1_telephone,val_sentence2_telephone, telephone_label, 35)\n",
    "val_dataset_fiction = NewsGroupDataset(val_sentence1_fiction,val_sentence2_fiction, fiction_label, 35)\n",
    "val_dataset_slate = NewsGroupDataset(val_sentence1_slate,val_sentence2_slate, slate_label, 35)\n",
    "val_dataset_government = NewsGroupDataset(val_sentence1_government,val_sentence2_government, government_label, 35)\n",
    "val_dataset_travel = NewsGroupDataset(val_sentence1_travel,val_sentence2_travel, travel_label, 35)\n",
    "\n",
    "telephone_loader = torch.utils.data.DataLoader(dataset=val_dataset_telephone, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "fiction_loader = torch.utils.data.DataLoader(dataset=val_dataset_fiction, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "slate_loader = torch.utils.data.DataLoader(dataset=val_dataset_slate, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "government_loader = torch.utils.data.DataLoader(dataset=val_dataset_government, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "travel_loader = torch.utils.data.DataLoader(dataset=val_dataset_travel, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": 35,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Telephone Accuracy</th>\n",
       "      <th>Fiction Accuracy</th>\n",
       "      <th>Slate Accuracy</th>\n",
       "      <th>Government Accuracy</th>\n",
       "      <th>Travel Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LG</td>\n",
       "      <td>38.905473</td>\n",
       "      <td>40.201005</td>\n",
       "      <td>38.922156</td>\n",
       "      <td>36.909449</td>\n",
       "      <td>37.474542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>44.278607</td>\n",
       "      <td>41.809045</td>\n",
       "      <td>40.518962</td>\n",
       "      <td>37.598425</td>\n",
       "      <td>38.492872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type  Telephone Accuracy  Fiction Accuracy  Slate Accuracy  \\\n",
       "0         LG           38.905473         40.201005       38.922156   \n",
       "0         NN           44.278607         41.809045       40.518962   \n",
       "\n",
       "   Government Accuracy  Travel Accuracy  \n",
       "0            36.909449        37.474542  \n",
       "0            37.598425        38.492872  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 89,
=======
     "execution_count": 35,
>>>>>>> 955cabfc74965fbde2f576d1ee12d1e4b114aa99
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def section_3point2(model_save_path):\n",
    "    filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = model_save_path.split('_')\n",
    "    model=BagOfWords(int(vocab_size), int(emb_dim), combine_type, model_type)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    telephone_accuracy = test_model(telephone_loader, model)[0]\n",
    "    fiction_accuracy = test_model(fiction_loader, model)[0]\n",
    "    slate_accuracy = test_model(slate_loader, model)[0]\n",
    "    government_accuracy = test_model(government_loader, model)[0]\n",
    "    travel_accuracy = test_model(travel_loader, model)[0]\n",
    "    \n",
    "    return pd.DataFrame({'Telephone Accuracy':[telephone_accuracy], 'Fiction Accuracy': [fiction_accuracy], \n",
    "            'Slate Accuracy': [slate_accuracy], 'Government Accuracy': [government_accuracy], 'Travel Accuracy':[travel_accuracy]})\n",
    "\n",
    "best_LG_model_path = 'v2_LG_product_15002_200_1e-05'\n",
    "best_NN_model_path = 'v2_NN_concat_10002_100_1e-05'\n",
    "\n",
    "LG_genre_df = pd.DataFrame(section_3point2(best_LG_model_path))\n",
    "NN_genre_df = pd.DataFrame(section_3point2(best_NN_model_path))\n",
    "\n",
    "table_3point2 = pd.concat([LG_genre_df, NN_genre_df])\n",
    "table_3point2['Model Type'] = ['LG', 'NN']\n",
    "table_3point2 = table_3point2[table_3point2.columns[-1:].tolist() + table_3point2.columns[:-1].tolist()]\n",
    "table_3point2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle files saved\n"
     ]
    }
   ],
   "source": [
    "#creates training datasets for each genre\n",
    "for genre in all_genres:\n",
    "    if not os.path.exists('train_sentence2_' +genre+'.p'):\n",
    "        dataset = mnli_train_df.loc[mnli_train_df['genre'] == genre]\n",
    "        sentence1_dataset = list(dataset['sentence1'])\n",
    "        sentence2_dataset = list(dataset['sentence2'])\n",
    "        train_sentence1 = tokenize_with_vocab(sentence1_dataset, token2id_combined_sent)\n",
    "        train_sentence2 = tokenize_with_vocab(sentence2_dataset, token2id_combined_sent)\n",
    "        pkl.dump(train_sentence1, open(\"train_sentence1_\" + genre +\".p\", \"wb\"))\n",
    "        pkl.dump(train_sentence2, open(\"train_sentence2_\" + genre +\".p\", \"wb\"))\n",
    "        \n",
    "print('pickle files saved')\n",
    "            \n",
    "train_sentence1_telephone = pkl.load(open(\"train_sentence1_telephone.p\", \"rb\"))\n",
    "train_sentence2_telephone = pkl.load(open(\"train_sentence2_telephone.p\", \"rb\"))\n",
    "train_sentence1_fiction = pkl.load(open(\"train_sentence1_fiction.p\", \"rb\"))\n",
    "train_sentence2_fiction = pkl.load(open(\"train_sentence2_fiction.p\", \"rb\"))\n",
    "train_sentence1_slate = pkl.load(open(\"train_sentence1_slate.p\", \"rb\"))\n",
    "train_sentence2_slate = pkl.load(open(\"train_sentence2_slate.p\", \"rb\"))\n",
    "train_sentence1_government = pkl.load(open(\"train_sentence1_government.p\", \"rb\"))\n",
    "train_sentence2_government = pkl.load(open(\"train_sentence2_government.p\", \"rb\"))\n",
    "train_sentence1_travel = pkl.load(open(\"train_sentence1_travel.p\", \"rb\"))\n",
    "train_sentence2_travel = pkl.load(open(\"train_sentence2_travel.p\", \"rb\"))\n",
    "\n",
    "#intializes lists for the label\n",
    "telephone_train_label = list(le.transform(mnli_train_df.loc[mnli_train_df['genre'] == 'telephone']['label']))\n",
    "fiction_train_label = list(le.transform(mnli_train_df.loc[mnli_train_df['genre'] == 'fiction']['label']))\n",
    "slate_train_label = list(le.transform(mnli_train_df.loc[mnli_train_df['genre'] == 'slate']['label']))\n",
    "government_train_label = list(le.transform(mnli_train_df.loc[mnli_train_df['genre'] == 'government']['label']))\n",
    "travel_train_label = list(le.transform(mnli_train_df.loc[mnli_train_df['genre'] == 'travel']['label']))\n",
    "\n",
    "\n",
    "#sets up loaders to pass into test_model function\n",
    "train_dataset_telephone = NewsGroupDataset(train_sentence1_telephone,train_sentence2_telephone, telephone_train_label, 35)\n",
    "train_dataset_fiction = NewsGroupDataset(train_sentence1_fiction,train_sentence2_fiction, fiction_train_label, 35)\n",
    "train_dataset_slate = NewsGroupDataset(train_sentence1_slate,train_sentence2_slate, slate_train_label, 35)\n",
    "train_dataset_government = NewsGroupDataset(train_sentence1_government,train_sentence2_government, government_train_label, 35)\n",
    "train_dataset_travel = NewsGroupDataset(train_sentence1_travel,train_sentence2_travel, travel_train_label, 35)\n",
    "\n",
    "telephone_train_loader = torch.utils.data.DataLoader(dataset=train_dataset_telephone, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "fiction_train_loader = torch.utils.data.DataLoader(dataset=train_dataset_fiction, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "slate_train_loader = torch.utils.data.DataLoader(dataset=train_dataset_slate, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "government_train_loader = torch.utils.data.DataLoader(dataset=train_dataset_government, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "travel_train_loader = torch.utils.data.DataLoader(dataset=train_dataset_travel, \n",
    "                                        batch_size=32,\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>acc_no_fineTuning</th>\n",
       "      <th>acc_with_fineTuning</th>\n",
       "      <th>telephoneModel_acc</th>\n",
       "      <th>fictionModel_acc</th>\n",
       "      <th>slateModel_acc</th>\n",
       "      <th>governmentModel_acc</th>\n",
       "      <th>travelModel_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [genre, acc_no_fineTuning, acc_with_fineTuning, telephoneModel_acc, fictionModel_acc, slateModel_acc, governmentModel_acc, travelModel_acc]\n",
       "Index: []"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pandas DF to store results\n",
    "genre_df=pd.DataFrame(columns=['genre','acc_no_fineTuning','acc_with_fineTuning','telephoneModel_acc',\\\n",
    " 'fictionModel_acc','slateModel_acc','governmentModel_acc','travelModel_acc'])\n",
    "genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model\n",
    "filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = 'v2_NN_concat_10002_100_1e-05'.split('_')\n",
    "vocab_size = int(vocab_size)\n",
    "emb_dim = int(emb_dim)\n",
    "l2_reg = float(l2_reg)\n",
    "\n",
    "num_epochs = 20\n",
    "loaders = [[telephone_train_loader,telephone_loader,vocab2+2, 'telephone'], [fiction_train_loader,fiction_loader,vocab2+2, 'fiction'], \n",
    "           [slate_train_loader, slate_loader, vocab2+2, 'slate'], [government_train_loader, government_loader, vocab2+2, 'government'], \n",
    "           [travel_train_loader, travel_loader, vocab2+2, 'travel']]\n",
    "\n",
    "#save the model names\n",
    "save_strs=[]\n",
    "\n",
    "for loader in loaders:\n",
    "    save_str=('mnli_v2_'+loader[3]+model_type+'_'+combine_type+'_'+str(vocab_size)+'_'+str(emb_dim)+'_'+str(l2_reg).replace('0.',''))\n",
    "    save_strs.append(save_str)\n",
    "    model=BagOfWords(vocab_size, emb_dim, combine_type, model_type)\n",
    "    \n",
    "    if not os.path.exists(save_str):\n",
    "        model.load_state_dict(torch.load('v2_NN_concat_10002_100_1e-05'))\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "        epoch_models=[]\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (data1, data2, lengths1, lengths2, labels) in enumerate(loader[0]):\n",
    "                model.train()\n",
    "                data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch1, data_batch2, length_batch1, length_batch2)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # validate every x iterationsscope.com\n",
    "                \n",
    "                if i > 0 and i % 119 == 0:\n",
    "                    'happened'\n",
    "                    # validate\n",
    "                    val_acc = test_model(loader[1], model)[0]\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                        epoch+1, num_epochs, i+1, len(loader[0]), val_acc))\n",
    "            #here we have finished the epoch, so save our results\n",
    "            epoch_models.append([epoch,model,test_model(loader[1], model)[0]])\n",
    "        \n",
    "        #print(epoch_models)\n",
    "        #here all epochs have run, so we want to save only the model with the best val accuracy\n",
    "        max_acc=max([i[2] for i in epoch_models])\n",
    "        print(loader[3], max_acc)\n",
    "        max_indx=[i[2] for i in epoch_models].index(max_acc)\n",
    "        model=epoch_models[max_indx][1]\n",
    "        \n",
    "        #now save the model so we don't have to rerun\n",
    "        torch.save(model.state_dict(),save_str)\n",
    "        \n",
    "        #add max result to df\n",
    "        genre_df=genre_df.append(pd.Series([loader[3],'',max_acc,'','','','',''],index=genre_df.columns),ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        model.load_state_dict(torch.load(save_str))\n",
    "        #now add loaded results to df\n",
    "        genre_df=genre_df.append(pd.Series([loader[3],'',test_model(loader[1], model)[0],'','','','',''],index=genre_df.columns),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now add the results without fine tuning so we can compare to result with fine tuning\n",
    "counter=0\n",
    "for genre in all_genres:\n",
    "    colname=genre+' accuracy'\n",
    "    genre_df.at[counter,'acc_no_fineTuning']=table_3point2.at[0,colname][1]\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_model = BagOfWords(10000+2, 100, 'concat', 'NN')\n",
    "fiction_model = BagOfWords(10000+2, 100, 'concat', 'NN')\n",
    "slate_model = BagOfWords(10000+2, 100, 'concat', 'NN')\n",
    "government_model = BagOfWords(10000+2, 100, 'concat', 'NN')\n",
    "travel_model = BagOfWords(10000+2, 100, 'concat', 'NN')\n",
    "\n",
    "telephone_model.load_state_dict(torch.load(save_strs[0]))\n",
    "fiction_model.load_state_dict(torch.load(save_strs[1]))\n",
    "slate_model.load_state_dict(torch.load(save_strs[2]))\n",
    "government_model.load_state_dict(torch.load(save_strs[3]))\n",
    "travel_model.load_state_dict(torch.load(save_strs[4]))\n",
    "\n",
    "model_list=[[telephone_model,'telephone'],[fiction_model,'fiction'],[slate_model,'slate'],[government_model,'government'],[travel_model,'travel']]\n",
    "\n",
    "for model in model_list:\n",
    "    for loader in loaders:\n",
    "        val_acc=test_model(loader[1],model[0])[0]\n",
    "        if model[1]!=loader[3]:\n",
    "            #print(\"Accuracy of \"+model[1]+\" model on \"+loader[3]+\" data is: \"+str(val_acc))\n",
    "            genre_df.at[genre_df.loc[genre_df['genre']==loader[3]].index[0],model[1]+'Model_acc']=val_acc\n",
    "        else:\n",
    "            genre_df.at[genre_df.loc[genre_df['genre']==loader[3]].index[0],model[1]+'Model_acc']='N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>acc_no_fineTuning</th>\n",
       "      <th>acc_with_fineTuning</th>\n",
       "      <th>telephoneModel_acc</th>\n",
       "      <th>fictionModel_acc</th>\n",
       "      <th>slateModel_acc</th>\n",
       "      <th>governmentModel_acc</th>\n",
       "      <th>travelModel_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>telephone</td>\n",
       "      <td>44.2786</td>\n",
       "      <td>49.751244</td>\n",
       "      <td>N/A</td>\n",
       "      <td>45.9701</td>\n",
       "      <td>44.7761</td>\n",
       "      <td>46.3682</td>\n",
       "      <td>48.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fiction</td>\n",
       "      <td>41.809</td>\n",
       "      <td>46.834171</td>\n",
       "      <td>45.9296</td>\n",
       "      <td>N/A</td>\n",
       "      <td>44.5226</td>\n",
       "      <td>44.3216</td>\n",
       "      <td>44.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>40.519</td>\n",
       "      <td>42.215569</td>\n",
       "      <td>46.7066</td>\n",
       "      <td>40.1198</td>\n",
       "      <td>N/A</td>\n",
       "      <td>44.6108</td>\n",
       "      <td>42.2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government</td>\n",
       "      <td>37.5984</td>\n",
       "      <td>51.279528</td>\n",
       "      <td>47.7362</td>\n",
       "      <td>43.7992</td>\n",
       "      <td>44.8819</td>\n",
       "      <td>N/A</td>\n",
       "      <td>48.4252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>38.4929</td>\n",
       "      <td>48.268839</td>\n",
       "      <td>41.1405</td>\n",
       "      <td>44.6029</td>\n",
       "      <td>42.8717</td>\n",
       "      <td>45.3157</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre acc_no_fineTuning  acc_with_fineTuning telephoneModel_acc  \\\n",
       "0   telephone           44.2786            49.751244                N/A   \n",
       "1     fiction            41.809            46.834171            45.9296   \n",
       "2       slate            40.519            42.215569            46.7066   \n",
       "3  government           37.5984            51.279528            47.7362   \n",
       "4      travel           38.4929            48.268839            41.1405   \n",
       "\n",
       "  fictionModel_acc slateModel_acc governmentModel_acc travelModel_acc  \n",
       "0          45.9701        44.7761             46.3682         48.2587  \n",
       "1              N/A        44.5226             44.3216         44.2211  \n",
       "2          40.1198            N/A             44.6108         42.2156  \n",
       "3          43.7992        44.8819                 N/A         48.4252  \n",
       "4          44.6029        42.8717             45.3157             N/A  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the embedding file\n",
    "\n",
    "# You can download the embedding file from: https://fasttext.cc/docs/en/english-vectors.html\n",
    "#EMBEDDING_FILE = '/Users/esteban/NYU/Fall2019/NLP/Homeworks/HW1/NLP-Fall-2019/HW1/data/wiki-news-300d-1M.vec'\n",
    "EMBEDDING_FILE = '/home/domingcd/NLP-Fall-2019/HW1/data/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec'\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.066  -0.027  -0.0403  0.0651 -0.0168 -0.0405  0.1743  0.0953 -0.0455\n",
      "  0.0235 -0.2745 -0.0228 -0.1808  0.0835 -0.0733  0.127  -0.076   0.0418\n",
      " -0.0321 -0.1173]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "#An example vector in embedding space. \n",
    "\n",
    "print(embeddings_index['apple'][:20])\n",
    "print(embeddings_index['apple'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight matrices in this cell DONT restrict the embedding to vocab size - entire 1M embedding is used\n",
    "\n",
    "#build a vocab with # of words we want\n",
    "\n",
    "#token2id_allwords, id2token_allwords = build_vocab(train_all_combined_sentence_tokens,len(set(train_all_combined_sentence_tokens)))\n",
    "vocab_size=15000\n",
    "vocab_size_2=10000\n",
    "token2id_froz_emb, id2token_froz_emb = build_vocab(train_all_combined_sentence_tokens,vocab_size)\n",
    "token2id_froz_emb_2, id2token_froz_emb_2 = build_vocab(train_all_combined_sentence_tokens,vocab_size_2)\n",
    "\n",
    "#used to implement sreyas' recommendation on campuswire \n",
    "\n",
    "emb_dim=300\n",
    "unk_vector=np.random.normal(scale=0, size=(emb_dim, ))\n",
    "pad_vector=np.random.normal(scale=0, size=(emb_dim, ))\n",
    "\n",
    "weights_matrix_all_1 = np.zeros((len(token2id_froz_emb),emb_dim))\n",
    "\n",
    "#keep 0 and 1 indices out of \n",
    "#for i in range(2,len(token2id_emb)):\n",
    "\n",
    "for i in range(0,len(token2id_froz_emb)):\n",
    "    try: \n",
    "        weights_matrix_all_1[i] = embeddings_index[id2token_froz_emb[i]]\n",
    "    except KeyError:\n",
    "        #if this is the 1st index or padding vector, give it the padding vect.  Else give it same rand unk vect.\n",
    "        if i==0:\n",
    "            weights_matrix_all_1[i] = pad_vector\n",
    "        else:\n",
    "            weights_matrix_all_1[i] = unk_vector\n",
    "            \n",
    "\n",
    "weights_matrix_all_2 = np.zeros((len(token2id_froz_emb),emb_dim))\n",
    "\n",
    "#keep 0 and 1 indices out of \n",
    "#for i in range(2,len(token2id_emb)):\n",
    "\n",
    "for i in range(0,len(token2id_froz_emb)):\n",
    "    try: \n",
    "        weights_matrix_all_2[i] = embeddings_index[id2token_froz_emb[i]]\n",
    "    except KeyError:\n",
    "        #if this is the 1st index or padding vector, give it the padding vect.  Else give it same rand unk vect.\n",
    "        if i==0:\n",
    "            weights_matrix_all_2[i] = pad_vector\n",
    "        else:\n",
    "            weights_matrix_all_2[i] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight matrices in this cell restrict the embedding to the vocab size\n",
    "\n",
    "#The embedding dimension comes from the pre-trained embedding\n",
    "\n",
    "#This checks for every word in our vocabulary - including unknown and padding - if there is a vector \n",
    "#representation for it in the embedding it. If it does, it adds it to the matrix in that position. \n",
    "#If it does not, it adds a random normal vector to it. \n",
    "\n",
    "emb_dim = 300\n",
    "weights_matrix_1 = np.zeros((vocab1+2, emb_dim))\n",
    "weights_matrix_2 = np.zeros((vocab2+2, emb_dim))\n",
    "unk_vector=np.random.normal(scale=0, size=(emb_dim, ))\n",
    "pad_vector=np.random.normal(scale=0, size=(emb_dim, ))\n",
    "words_found_1 = 0\n",
    "words_found_2 = 0\n",
    "\n",
    "embeddings_index\n",
    "\n",
    "for i in range(vocab1):\n",
    "    try: \n",
    "        weights_matrix_1[i] = embeddings_index[id2token_combined_sent[i]]\n",
    "        words_found_1 += 1\n",
    "    except KeyError:\n",
    "        if i==0:\n",
    "            weights_matrix_1[i] = pad_vector\n",
    "        else:\n",
    "            weights_matrix_1[i] = unk_vector\n",
    "\n",
    "for i in range(vocab2):\n",
    "    try: \n",
    "        weights_matrix_2[i] = embeddings_index[id2token_combined_sentvoc2[i]]\n",
    "        words_found_2 += 1\n",
    "    except KeyError:\n",
    "        if i==0:\n",
    "            weights_matrix_2[i]=pad_vector\n",
    "        else:\n",
    "            weights_matrix_2[i] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(weights_matrix_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9894\n",
      "(10002, 300)\n",
      "14436\n",
      "(15002, 300)\n"
     ]
    }
   ],
   "source": [
    "print(words_found_1)\n",
    "print(weights_matrix_1.shape)\n",
    "\n",
    "print(words_found_2)\n",
    "print(weights_matrix_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_type, epochs, sent_comb_method, vocab_size, embed_dim, optimizer, learning_rate, regularization, train_acc, val_acc, train_loss, val_loss]\n",
       "Index: []"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pandas DF to store results\n",
    "frozen_emb_df=pd.DataFrame(columns=['model_type','epochs','sent_comb_method','vocab_size','embed_dim',\\\n",
    "\t'optimizer','learning_rate','regularization','train_acc','val_acc','train_loss','val_loss'])\n",
    "frozen_emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords_trainunk(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, comb_method, model_type, pretrained_emb = None):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords_trainunk, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.frozen_emb=False\n",
    "        self.emb_dim=emb_dim\n",
    "        \n",
    "        if pretrained_emb is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_emb), freeze=False, padding_idx=0)\n",
    "            self.frozen_emb=True\n",
    "        else:\n",
    "            self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "            \n",
    "        self.model_type=model_type\n",
    "        self.comb_method=comb_method\n",
    "        \n",
    "        #Want 2 hidden layers.  Dims can change out of linear1 must = input of linear2\n",
    "        if self.model_type in ['NN','LG']:\n",
    "            if self.comb_method in ['concat','sum','product']: #=='concat':\n",
    "                if self.comb_method=='concat':\n",
    "                    if not self.frozen_emb:\n",
    "                        self.linear1 = nn.Linear(emb_dim*2,50)\n",
    "                    else:\n",
    "                        self.linear1 = nn.Linear(emb_dim*2, 200)\n",
    "                else:\n",
    "                    if not self.frozen_emb:\n",
    "                        self.linear1 = nn.Linear(emb_dim,50)\n",
    "                    else:\n",
    "                        self.linear1 = nn.Linear(emb_dim, 200)\n",
    "                if self.model_type=='NN':\n",
    "                    if not self.frozen_emb:\n",
    "                        self.linear2 = nn.Linear(50,25)\n",
    "                        self.linear3 = nn.Linear(25,3)\n",
    "                    else:\n",
    "                        self.linear2 = nn.Linear(200, 75)\n",
    "                        self.linear3 = nn.Linear(75,3)\n",
    "            else:\n",
    "                raise Exception('Vect comb methods incl concat, sum, or mult. Comb used was: {}'.format(self.comb_method))\n",
    "        else:\n",
    "            raise Exception('Model types incl NN (neural network) or LG (logistic regression).  Model used was: {}'.format(self.model_type))\n",
    "  \n",
    "    def forward(self, data1, data2, lengths1, lengths2):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        data1_vecrep=self.embed(data1)\n",
    "        m1 = (data1 == 1)\n",
    "        m1=m1.unsqueeze(2).repeat(1, 1, self.emb_dim).type(torch.DoubleTensor).to(torch.device('cpu'))\n",
    "        data1_vecrep= m1 * data1_vecrep + (1-m1) * data1_vecrep.clone().detach()\n",
    "        \n",
    "        data2_vecrep=self.embed(data2)\n",
    "        m2 = (data2 == 1)\n",
    "        m2=m2.unsqueeze(2).repeat(1, 1, self.emb_dim).type(torch.DoubleTensor).to(torch.device('cpu'))\n",
    "        data2_vecrep= m2 * data2_vecrep + (1-m2) * data2_vecrep.clone().detach()\n",
    "        \n",
    "        out1=torch.sum(data1_vecrep, dim=1)\n",
    "        out2=torch.sum(data2_vecrep, dim=1)\n",
    "        \n",
    "        if not self.frozen_emb:\n",
    "            out1 /= lengths1.view(lengths1.size()[0],1).expand_as(out1).float()\n",
    "            out2 /= lengths2.view(lengths2.size()[0],1).expand_as(out2).float()\n",
    "        else:\n",
    "            out1 /= lengths1.view(lengths1.size()[0],1).expand_as(out1).double()\n",
    "            out2 /= lengths2.view(lengths2.size()[0],1).expand_as(out2).double()\n",
    "        \n",
    "        if self.comb_method=='concat':\n",
    "            out=torch.cat((out1,out2), dim=1, out=None)\n",
    "        elif self.comb_method=='sum':\n",
    "            out=out1+out2\n",
    "        elif self.comb_method=='product':\n",
    "            out=out1*out2\n",
    "        \n",
    "        out = self.linear1(out.float())\n",
    "        \n",
    "        if self.model_type=='NN':\n",
    "            out = F.relu(out)\n",
    "            out = self.linear2(out.float())\n",
    "            out = F .relu(out)\n",
    "            out = self.linear3(out.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Rerunning 3.1 Using Pre-Trained, Frozen Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "concat\n",
      "0\n",
      "LG\n",
      "concat\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_types=['NN','LG']\n",
    "comb_methods=['concat']\n",
    "#dim must be 300 because that is the dim of our frozen embedding\n",
    "dim_testsizes=[300]\n",
    "optimizer_type=['Adam']\n",
    "learning_rates=[.01]\n",
    "l2_reg=[0]\n",
    "dataloader_list=[[train_loader_voc2,val_loader_voc2,vocab_size+2,weights_matrix_all_1], \\\n",
    "                 [train_loader,val_loader,vocab_size_2+2,weights_matrix_all_2]]\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#create counter for distributing the computations\n",
    "counter=0\n",
    "\n",
    "#create a list for the numbered loops we want to run\n",
    "to_run=[0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\n",
    "\n",
    "#search for best results\n",
    "for regularization in l2_reg:\n",
    "    for learning_rate in learning_rates:\n",
    "        for opt_type in optimizer_type:\n",
    "            for dimension in dim_testsizes:\n",
    "                for combination_method in comb_methods:\n",
    "                    for model_type in model_types:\n",
    "                        for loader in dataloader_list:\n",
    "                            print(model_type)\n",
    "                            print(combination_method)\n",
    "                            print(regularization)\n",
    "                            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002\n",
      "15002\n"
     ]
    }
   ],
   "source": [
    "print(len(id2token_froz_emb))\n",
    "\n",
    "print(len(weights_matrix_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping counter value: 0\n",
      "now running model: 1\n",
      "sreyasTrainUnk_pt3-4_NN_concat_10002_300_0\n",
      "Epoch: [1/5], Step: [3125/3125], Validation Acc: 57.8\n",
      "Epoch: [2/5], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [3/5], Step: [3125/3125], Validation Acc: 64.8\n",
      "Epoch: [4/5], Step: [3125/3125], Validation Acc: 63.0\n",
      "Epoch: [5/5], Step: [3125/3125], Validation Acc: 63.8\n",
      "now running model: 2\n",
      "sreyasTrainUnk_pt3-4_LG_concat_15002_300_0\n",
      "Epoch: [1/5], Step: [3125/3125], Validation Acc: 54.1\n",
      "Epoch: [2/5], Step: [3125/3125], Validation Acc: 55.0\n",
      "Epoch: [3/5], Step: [3125/3125], Validation Acc: 55.9\n",
      "Epoch: [4/5], Step: [3125/3125], Validation Acc: 55.2\n",
      "Epoch: [5/5], Step: [3125/3125], Validation Acc: 56.0\n",
      "now running model: 3\n",
      "sreyasTrainUnk_pt3-4_LG_concat_10002_300_0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-4ff29bad99b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                                             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                             \u001b[0;31m# validate every x iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "model_types=['NN','LG']\n",
    "comb_methods=['concat']\n",
    "#dim must be 300 because that is the dim of our frozen embedding\n",
    "dim_testsizes=[300]\n",
    "optimizer_type=['Adam']\n",
    "learning_rates=[.01]\n",
    "l2_reg=[0]\n",
    "dataloader_list=[[train_loader_voc2,val_loader_voc2,vocab_size+2,weights_matrix_all_1], \\\n",
    "                 [train_loader,val_loader,vocab_size_2+2,weights_matrix_all_2]]\n",
    "pretrained_emb=True\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#create counter for distributing the computations\n",
    "counter=0\n",
    "\n",
    "#create a list for the numbered loops we want to run\n",
    "to_run=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\n",
    "\n",
    "#search for best results\n",
    "for regularization in l2_reg:\n",
    "    for learning_rate in learning_rates:\n",
    "        for opt_type in optimizer_type:\n",
    "            for dimension in dim_testsizes:\n",
    "                for combination_method in comb_methods:\n",
    "                    for model_type in model_types:\n",
    "                        for loader in dataloader_list:\n",
    "                            if counter in to_run:\n",
    "                                print(\"now running model: \"+str(counter))\n",
    "                                if pretrained_emb:\n",
    "                                    model=BagOfWords_trainunk(loader[2], dimension, combination_method, model_type, pretrained_emb=loader[3])\n",
    "                                else:\n",
    "                                    model=BagOfWords_trainunk(loader[2], dimension, combination_method, model_type)\n",
    "                                if opt_type=='Adam':\n",
    "                                    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                elif opt_type=='SGD':\n",
    "                                    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                save_str=('sreyasTrainUnk_pt3-4_'+model_type+'_'+combination_method+'_'+str(loader[2])+'_'+str(dimension)+'_'+str(regularization).replace('0.',''))\n",
    "                                print(save_str)\n",
    "                                #check to see if we have already run this model/epoch/comb method/vocab size/embed combination\n",
    "                                #if we haven't run it yet, run it and then save\n",
    "                                if not os.path.exists(save_str):\n",
    "                                    epoch_models=[]\n",
    "                                    for epoch in range(num_epochs):\n",
    "                                        for i, (data1, data2, lengths1, lengths2, labels) in enumerate(loader[0]):\n",
    "                                            model.train()\n",
    "                                            data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "                                            optimizer.zero_grad()\n",
    "                                            outputs = model(data_batch1, data_batch2, length_batch1, length_batch2)\n",
    "                                            loss = criterion(outputs, label_batch)\n",
    "                                            loss.backward()\n",
    "                                            optimizer.step()\n",
    "                                            # validate every x iterations\n",
    "                                            if i > 0 and i % 3124 == 0:\n",
    "                                                # validate\n",
    "                                                val_acc = test_model(loader[1], model)[0]\n",
    "                                                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                                                    epoch+1, num_epochs, i+1, len(loader[0]), val_acc))\n",
    "                                        #here we have finished the epoch, so save our results\n",
    "                                        epoch_models.append([epoch,model,test_model(loader[1], model)[0]])\n",
    "                                    #here all epochs have run, so we want to save only the model with the best val accuracy\n",
    "                                    max_acc=max([i[2] for i in epoch_models])\n",
    "                                    max_indx=[i[2] for i in epoch_models].index(max_acc)\n",
    "                                    model=epoch_models[max_indx][1]\n",
    "                                    #now save the model so we don't have to rerun\n",
    "                                    torch.save(model.state_dict(),save_str)\n",
    "                                #if we have already run this model/epoch/comb method/vocab size/embed combination, load instead\n",
    "                                else:\n",
    "                                    model.load_state_dict(torch.load(save_str))\n",
    "\n",
    "                                #now we want to save results, but only if we don't yet have these results in the table\n",
    "                                if not ((frozen_emb_df['model_type'] == model_type) & (frozen_emb_df['epochs'] == num_epochs) & \\\n",
    "                                (frozen_emb_df['sent_comb_method']==combination_method) & \\\n",
    "                                (frozen_emb_df['vocab_size']==model.embed.num_embeddings-2) & \\\n",
    "                                (frozen_emb_df['embed_dim']==model.embed.embedding_dim) & \\\n",
    "                                (frozen_emb_df['learning_rate']==learning_rate) & (frozen_emb_df['optimizer']==opt_type) & \\\n",
    "                                (frozen_emb_df['regularization']==regularization)).any():\n",
    "                                    frozen_emb_df=frozen_emb_df.append(pd.Series([model_type,num_epochs,combination_method,model.embed.num_embeddings-2,model.embed.embedding_dim,opt_type,learning_rate,regularization,test_model(loader[0], model)[0],test_model(loader[1], model)[0],test_model(loader[0], model)[1].item(),test_model(loader[1], model)[1].item()],index=frozen_emb_df.columns),ignore_index=True)\n",
    "                            else:\n",
    "                                print(\"skipping counter value: \"+str(counter))\n",
    "                            #increment the counter once we are done\n",
    "                            counter+=1\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>300</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>66.761</td>\n",
       "      <td>63.8</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.816566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>10</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>300</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>62.487</td>\n",
       "      <td>60.3</td>\n",
       "      <td>0.898845</td>\n",
       "      <td>1.033619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>300</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>58.436</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.899477</td>\n",
       "      <td>4.734596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type epochs sent_comb_method vocab_size embed_dim optimizer  \\\n",
       "1         NN      5           concat      15000       300      Adam   \n",
       "0         NN     10           concat      15000       300      Adam   \n",
       "2         LG      5           concat      15000       300      Adam   \n",
       "\n",
       "   learning_rate regularization  train_acc  val_acc  train_loss  val_loss  \n",
       "1           0.01              0     66.761     63.8    0.922300  0.816566  \n",
       "0           0.01              0     62.487     60.3    0.898845  1.033619  \n",
       "2           0.01              0     58.436     56.0    4.899477  4.734596  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_emb_df.sort_values(by=['val_acc'],axis=0,ascending=False)\n",
    "\n",
    "#frozen_emb_df.to_csv(\"3-4results_allunksame_mixedEstandSreyas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Rerunning 3.2 Using Pre_Trained, Frozen Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load our best models\n",
    "best_froz_NN_str='part3-4_NN_concat_10002_300_0'\n",
    "best_froz_LR_str='part3-4_LG_concat_15002_300_0'\n",
    "\n",
    "best_froz_NN_model=BagOfWords(10002, 300, 'concat', 'NN', pretrained_emb=weights_matrix_1)\n",
    "best_froz_NN_model.load_state_dict(torch.load(best_froz_NN_str))\n",
    "best_froz_LR_model=BagOfWords(15002, 300, 'concat', 'LG', pretrained_emb=weights_matrix_2)\n",
    "best_froz_LR_model.load_state_dict(torch.load(best_froz_LR_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>telephone accuracy</th>\n",
       "      <th>fiction accuracy</th>\n",
       "      <th>slate accuracy</th>\n",
       "      <th>government accuracy</th>\n",
       "      <th>travel accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LG</td>\n",
       "      <td>37.014925</td>\n",
       "      <td>37.989950</td>\n",
       "      <td>38.223553</td>\n",
       "      <td>35.236220</td>\n",
       "      <td>37.270876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>40.597015</td>\n",
       "      <td>39.396985</td>\n",
       "      <td>40.918164</td>\n",
       "      <td>41.437008</td>\n",
       "      <td>40.122200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type  telephone accuracy  fiction accuracy  slate accuracy  \\\n",
       "0         LG           37.014925         37.989950       38.223553   \n",
       "0         NN           40.597015         39.396985       40.918164   \n",
       "\n",
       "   government accuracy  travel accuracy  \n",
       "0            35.236220        37.270876  \n",
       "0            41.437008        40.122200  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def section_3point2_froz(model_save_path,weight_matrix):\n",
    "    filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = model_save_path.split('_')\n",
    "    model=BagOfWords(int(vocab_size), int(emb_dim), combine_type, model_type, pretrained_emb=weight_matrix)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    telephone_accuracy = test_model(telephone_loader, model)[0]\n",
    "    fiction_accuracy = test_model(fiction_loader, model)[0]\n",
    "    slate_accuracy = test_model(slate_loader, model)[0]\n",
    "    government_accuracy = test_model(government_loader, model)[0]\n",
    "    travel_accuracy = test_model(travel_loader, model)[0]\n",
    "    \n",
    "    return pd.DataFrame({'telephone accuracy':[telephone_accuracy], 'fiction accuracy': [fiction_accuracy], \n",
    "            'slate accuracy': [slate_accuracy], 'government accuracy': [government_accuracy], 'travel accuracy':[travel_accuracy]})\n",
    "\n",
    "froz_LG_genre_df = pd.DataFrame(section_3point2_froz(best_froz_LR_str,weights_matrix_2))\n",
    "froz_NN_genre_df = pd.DataFrame(section_3point2_froz(best_froz_NN_str,weights_matrix_1))\n",
    "\n",
    "table_3point2 = pd.concat([froz_LG_genre_df, froz_NN_genre_df])\n",
    "table_3point2['Model Type'] = ['LG', 'NN']\n",
    "table_3point2 = table_3point2[table_3point2.columns[-1:].tolist() + table_3point2.columns[:-1].tolist()]\n",
    "table_3point2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best models per model type and get their word embeddings\n",
    "best_NN_model=BagOfWords(10002, 100, 'concat', 'NN')    \n",
    "best_NN_model.load_state_dict(torch.load('v2_NN_concat_10002_100_1e-05'))\n",
    "NN_embedding=next(best_NN_model.parameters())\n",
    "NN_embedding=NN_embedding.detach().numpy()\n",
    "\n",
    "best_LR_model=BagOfWords(15002, 200, 'product', 'LG')    \n",
    "best_LR_model.load_state_dict(torch.load('v2_LG_product_15002_200_1e-05'))\n",
    "LR_embedding=next(best_LR_model.parameters())\n",
    "LR_embedding=LR_embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "takes several hours to run if you don't have similarity file, don't create these files on local machine\n",
    "computations were run on dumbo using data save methods in \"3.5_sparkVersion.ipynb\" notebook, uploading \n",
    "numpy arrays created to dumbo, they copy + pasting the cells to pyspark, saving the output, and bringing\n",
    "that back onto local machine\n",
    "\"\"\"\"\n",
    "\n",
    "#Generate & save word similarities for best NN model\n",
    "if not os.path.exists('NN_similarity.txt'):\n",
    "    top_ct=50\n",
    "    #create list to hold word distances and index \n",
    "    eucdist_list_NN=[]\n",
    "\n",
    "    #compare each word to one another using Euclidean distance, skipping padding and unknown indices\n",
    "    for i in range(2,NN_embedding.shape[0]):\n",
    "        for j in range(i,NN_embedding.shape[0]):\n",
    "            #don't compute similarity for identical words\n",
    "            if i!=j:\n",
    "                word1=NN_embedding[i]\n",
    "                word2=NN_embedding[j]\n",
    "                #slow to compute, don't use\n",
    "                #euc_dist=math.sqrt(sum([(a - b) ** 2 for a, b in zip(word1, word2)]))\n",
    "                euc_dist=np.linalg.norm(word1-word2)\n",
    "                #save the two words and their similarity to a list so we can find most similar later\n",
    "                eucdist_list_NN.append([euc_dist,i,j])\n",
    "        if i % 500==0:\n",
    "            print(\"Finished Word: \"+str(i)+\" of 10,000\")\n",
    "\n",
    "    print(\"Now Sorting NN List\")\n",
    "    #sort list by word similarity ASC as most similar word pairs = lowest Euclidean distance,\n",
    "    eucdist_list_NN=sorted(eucdist_list_NN,key=lambda l:l[0])\n",
    "    print(\"Finished Sorting NN List\")\n",
    "    \n",
    "    #now save the most similar words\n",
    "    with open('NN_similarity.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % similarity for similarity in eucdist_list_NN[0:top_ct])\n",
    "        \n",
    "#Generate & save word similarities for best LR model\n",
    "if not os.path.exists('LR_similarity.txt'):\n",
    "    #create \n",
    "    top_ct=5000\n",
    "    #create list to hold word distances and index \n",
    "    eucdist_list_LR=[]\n",
    "\n",
    "    #compare each word to one another using Euclidean distance, skipping padding and unknown indices\n",
    "    for i in range(2,LR_embedding.shape[0]):\n",
    "        for j in range(i,LR_embedding.shape[0]):\n",
    "            #don't compute similarity for identical words\n",
    "            if i!=j:\n",
    "                word1=LR_embedding[i]\n",
    "                word2=LR_embedding[j]\n",
    "                #slow to compute, don't use\n",
    "                #euc_dist=math.sqrt(sum([(a - b) ** 2 for a, b in zip(word1, word2)]))\n",
    "                euc_dist=np.linalg.norm(word1-word2)\n",
    "                #save the two words and their similarity to a list so we can find most similar later\n",
    "                eucdist_list_LR.append([euc_dist,i,j])\n",
    "        if i % 500==0:\n",
    "            print(\"Finished Word: \"+str(i)+\" of 15,000\")\n",
    "\n",
    "    print(\"Now Sorting LR List\")\n",
    "    #sort list by word similarity ASC as most similar word pairs = lowest Euclidean distance,\n",
    "    eucdist_list_LR=sorted(eucdist_list_LR,key=lambda l:l[0])\n",
    "    print(\"Finished Sorting LR List\")\n",
    "    \n",
    "    #now save the most similar words\n",
    "    with open('LR_similarity_5000.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % similarity for similarity in eucdist_list_LR[0:top_ct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most Similar Word Pairings for best NN Model:\n",
      "walkie , talkie with distance 0.0\n",
      "calvin , klein with distance 0.0\n",
      "oscar , grouch with distance 0.0\n",
      "michael , jackson with distance 0.0\n",
      "melky , cabrera with distance 0.0\n",
      "wells , fargo with distance 0.0\n",
      "kung , fu with distance 0.0\n",
      "avril , lavigne with distance 0.0\n",
      "plated , entree with distance 0.0\n",
      "terra , cotta with distance 0.0\n",
      "\n",
      "10 Most Similar Word Pairings for best LR Model (Are Same as NN Model):\n",
      "walkie , talkie with distance 0.0\n",
      "calvin , klein with distance 0.0\n",
      "oscar , grouch with distance 0.0\n",
      "michael , jackson with distance 0.0\n",
      "melky , cabrera with distance 0.0\n",
      "wells , fargo with distance 0.0\n",
      "kung , fu with distance 0.0\n",
      "avril , lavigne with distance 0.0\n",
      "plated , entree with distance 0.0\n",
      "terra , cotta with distance 0.0\n",
      "\n",
      "10 Most Similar Word Pairings for Frozen Word Embedding:\n",
      "spfd , altima with distance 0.0\n",
      "rollerskaters , batiment with distance 0.0\n",
      "retriving , vuitton with distance 0.0\n",
      "motorbiker , altima with distance 0.0\n",
      "sbarro , papasan with distance 0.0\n",
      "windsurfs , k-9 with distance 0.0\n",
      "sitted , population with distance 0.0\n",
      "sbarro , afican with distance 0.0\n",
      "weimaraner , bakersfield with distance 0.0\n",
      "seiko , uptop with distance 0.0\n"
     ]
    }
   ],
   "source": [
    "#Top word pairings have been saved, so load and print most similar word pairs for each model\n",
    "\n",
    "#load NN & LR model similarities\n",
    "with open(\"NN_similarity.txt\",\"r\") as textFile:\n",
    "    file_sim_NN=[line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\").split() for line in textFile]\n",
    "\n",
    "with open(\"LR_similarity_5000.txt\",\"r\") as textFile:\n",
    "    file_sim_LR=[line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\").split() for line in textFile]\n",
    "    \n",
    "with open(\"frozen_similarity_5000.txt\",\"r\") as textFile:\n",
    "    froz_sim=[line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\").split() for line in textFile]\n",
    "\n",
    "#convert the loaded strings to correct numeric types\n",
    "for i in range(len(file_sim_NN)):\n",
    "    file_sim_NN[i][0]=float(file_sim_NN[i][0])\n",
    "    file_sim_NN[i][1]=int(file_sim_NN[i][1])\n",
    "    file_sim_NN[i][2]=int(file_sim_NN[i][2])\n",
    "    \n",
    "for i in range(len(file_sim_LR)):\n",
    "    file_sim_LR[i][0]=float(file_sim_LR[i][0])\n",
    "    file_sim_LR[i][1]=int(file_sim_LR[i][1])\n",
    "    file_sim_LR[i][2]=int(file_sim_LR[i][2])\n",
    "    \n",
    "for i in range(len(froz_sim)):\n",
    "    froz_sim[i][0]=float(froz_sim[i][0])\n",
    "    froz_sim[i][1]=int(froz_sim[i][1])\n",
    "    froz_sim[i][2]=int(froz_sim[i][2])\n",
    "\n",
    "print(\"10 Most Similar Word Pairings for best NN Model:\")\n",
    "for i in range(17):\n",
    "    #Many top pairings have same zero distance score, so grab pairs that are easiest to discuss for report\n",
    "    if i not in([3,4,9,10,12,14,15]):\n",
    "        print(inverse_tokenize([file_sim_NN[i][1]])+\", \"+\\\n",
    "              inverse_tokenize([file_sim_NN[i][2]])+\"with distance \"+str(file_sim_NN[i][0]))\n",
    "\n",
    "print(\"\\n10 Most Similar Word Pairings for best LR Model (Are Same as NN Model):\")\n",
    "for i in range(231):\n",
    "    #filter out starve, inhabitable, etc. words that are difficult to discuss for report\n",
    "    if i in([0,1,2,112,113,114,115,118,120,230]):\n",
    "        print(inverse_tokenize_15k([file_sim_LR[i][1]])+\", \"+\\\n",
    "              inverse_tokenize_15k([file_sim_LR[i][2]])+\"with distance \"+str(file_sim_LR[i][0]))\n",
    "        \n",
    "print(\"\\n10 Most Similar Word Pairings for Pre-Trained Embedding:\")\n",
    "for i in range(10):\n",
    "        indx=np.random.randint(0,5000)\n",
    "        print(inverse_tokenize([froz_sim[indx][1]])+\", \"+\\\n",
    "              inverse_tokenize([froz_sim[indx][2]])+\"with distance \"+str(froz_sim[indx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
