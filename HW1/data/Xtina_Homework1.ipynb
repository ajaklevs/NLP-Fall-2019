{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from sklearn import preprocessing\n",
    "import os.path\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the data\n",
    "train_df = pd.read_csv('snli_train.tsv', sep=\"\\t\")\n",
    "val_df = pd.read_csv('snli_val.tsv',sep=\"\\t\")\n",
    "\n",
    "#get data & convert sentences to lists\n",
    "train_sentence1=train_df['sentence1'].values.tolist()\n",
    "train_sentence2=train_df['sentence2'].values.tolist()\n",
    "val_sentence1=val_df['sentence1'].values.tolist()\n",
    "val_sentence2=val_df['sentence2'].values.tolist()\n",
    "\n",
    "#convert the text labels to numeric\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "train_targets=le.transform(train_df['label']).tolist()\n",
    "val_targets=le.transform(val_df['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set tokens\n",
    "if not os.path.exists('train_all_combined_sentence_tokens.p'):\n",
    "    print(\"Tokenizing train data\")\n",
    "    train_sentence1_tokens, train_all_sentence1_tokens = tokenize_dataset(train_sentence1)\n",
    "    train_sentence2_tokens, train_all_sentence2_tokens = tokenize_dataset(train_sentence2)\n",
    "    train_all_combined_sentence_tokens = train_all_sentence1_tokens + train_all_sentence2_tokens\n",
    "    pkl.dump(train_all_combined_sentence_tokens, open(\"train_all_combined_sentence_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pickle files do not exist, collect them.  Otherwise, this if block will not run\n",
    "if not os.path.exists('train_sentence1_tokens.p'):\n",
    "    print (\"Tokenizing train data\")\n",
    "    train_sentence1_tokens, train_all_sentence1_tokens = tokenize_dataset(train_sentence1)\n",
    "    train_sentence2_tokens, train_all_sentence2_tokens = tokenize_dataset(train_sentence2)\n",
    "    pkl.dump(train_sentence1_tokens, open(\"train_sentence1_tokens.p\", \"wb\"))\n",
    "    pkl.dump(train_sentence2_tokens, open(\"train_sentence2_tokens.p\", \"wb\"))\n",
    "    #pkl.dump(train_all_sentence1_tokens, open(\"train_all_sentence1_tokens.p\", \"wb\"))\n",
    "    #pkl.dump(train_all_sentence2_tokens, open(\"train_all_sentence2_tokens.p\", \"wb\"))1\n",
    "\n",
    "    #combine tokens from both sentences to create a shared dictionary\n",
    "    train_all_combined_sentence_tokens = train_all_sentence1_tokens + train_all_sentence2_tokens\n",
    "    pkl.dump(train_all_combined_sentence_tokens, open(\"train_all_combined_sentence_tokens.p\", \"wb\"))\n",
    "\n",
    "    #val set tokens\n",
    "    print (\"Tokenizing val data\")\n",
    "    val_sentence1_tokens, _ = tokenize_dataset(val_sentence1)\n",
    "    val_sentence2_tokens, _ = tokenize_dataset(val_sentence2)\n",
    "    pkl.dump(val_sentence1_tokens, open(\"val_sentence1_tokens.p\", \"wb\"))\n",
    "    pkl.dump(val_sentence2_tokens, open(\"val_sentence2_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have previously run the previous cell, run this cell instead to load preprocessed datasets\n",
    "train_sentence1_tokens = pkl.load(open(\"train_sentence1_tokens.p\", \"rb\"))\n",
    "train_sentence2_tokens = pkl.load(open(\"train_sentence2_tokens.p\", \"rb\"))\n",
    "train_all_combined_sentence_tokens = pkl.load(open(\"train_all_combined_sentence_tokens.p\", \"rb\"))\n",
    "train_all_sentence1_tokens = pkl.load(open(\"train_all_sentence1_tokens.p\", \"rb\"))\n",
    "train_all_sentence2_tokens = pkl.load(open(\"train_all_sentence2_tokens.p\", \"rb\"))\n",
    "val_sentence1_tokens = pkl.load(open(\"val_sentence1_tokens.p\", \"rb\"))\n",
    "val_sentence2_tokens = pkl.load(open(\"val_sentence2_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n",
      "\n",
      "Total number of tokens in sentence1 train dataset is 1294135\n",
      "Total number of tokens in sentence2 train dataset is 743372\n",
      "Total number of tokens in combined sent1 & sent2 train dataset is 2037507\n",
      "\n",
      "Total number of *unique* tokens in sentence1 train dataset is 14131\n",
      "Total number of *unique* tokens in sentence2 train dataset is 15225\n",
      "Total number of *unique* tokens in sent1 & sent2 train dataset is 19642\n"
     ]
    }
   ],
   "source": [
    "#print information about the token datasets\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_tokens)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_tokens)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_tokens)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_tokens)))\n",
    "\n",
    "print (\"\\nTotal number of tokens in sentence1 train dataset is {}\".format(len(train_all_sentence1_tokens)))\n",
    "print (\"Total number of tokens in sentence2 train dataset is {}\".format(len(train_all_sentence2_tokens)))\n",
    "print (\"Total number of tokens in combined sent1 & sent2 train dataset is {}\".format(len(train_all_combined_sentence_tokens)))\n",
    "\n",
    "print (\"\\nTotal number of *unique* tokens in sentence1 train dataset is {}\".format(len(set(train_all_sentence1_tokens))))\n",
    "print (\"Total number of *unique* tokens in sentence2 train dataset is {}\".format(len(set(train_all_sentence2_tokens))))\n",
    "print (\"Total number of *unique* tokens in sent1 & sent2 train dataset is {}\".format(len(set(train_all_combined_sentence_tokens))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocabularies for sentence1 and sentence2\n",
    "from collections import Counter\n",
    "\n",
    "#max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#try different vocab sizes\n",
    "vocab1=10000\n",
    "vocab2=15000\n",
    "#vocab3=19000\n",
    "token2id_combined_sent, id2token_combined_sent = build_vocab(train_all_combined_sentence_tokens,vocab1)\n",
    "token2id_combined_sent_voc2, id2token_combined_sentvoc2 = build_vocab(train_all_combined_sentence_tokens,vocab2)\n",
    "#token2id_combined_sent_voc3, id2token_combined_sentvoc3 = build_vocab(train_all_combined_sentence_tokens,vocab3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 1983 ; token doll\n",
      "Token doll; token id 1983\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random \n",
    "\n",
    "random_token_id = random.randint(0, len(id2token_combined_sent)-1)\n",
    "random_token = id2token_combined_sent[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_combined_sent[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_combined_sent[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset.  After running this cell we will have converted the word tokens to indices\n",
    "def token2index_dataset(tokens_data,token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "#create train & val for vocab1 size\n",
    "train_sentence1_data_indices = token2index_dataset(train_sentence1_tokens,token2id_combined_sent)\n",
    "train_sentence2_data_indices = token2index_dataset(train_sentence2_tokens,token2id_combined_sent)\n",
    "val_sentence1_data_indices = token2index_dataset(val_sentence1_tokens,token2id_combined_sent)\n",
    "val_sentence2_data_indices = token2index_dataset(val_sentence2_tokens,token2id_combined_sent)\n",
    "\n",
    "#create train & val for vocab2 size\n",
    "train_sentence1_data_indices_voc2 = token2index_dataset(train_sentence1_tokens,token2id_combined_sent_voc2)\n",
    "train_sentence2_data_indices_voc2 = token2index_dataset(train_sentence2_tokens,token2id_combined_sent_voc2)\n",
    "val_sentence1_data_indices_voc2 = token2index_dataset(val_sentence1_tokens,token2id_combined_sent_voc2)\n",
    "val_sentence2_data_indices_voc2 = token2index_dataset(val_sentence2_tokens,token2id_combined_sent_voc2)\n",
    "\n",
    "#create train & val for vocab3 size\n",
    "# train_sentence1_data_indices_voc3 = token2index_dataset(train_sentence1_tokens,token2id_combined_sent_voc3)\n",
    "# train_sentence2_data_indices_voc3 = token2index_dataset(train_sentence2_tokens,token2id_combined_sent_voc3)\n",
    "# val_sentence1_data_indices_voc3 = token2index_dataset(val_sentence1_tokens,token2id_combined_sent_voc3)\n",
    "# val_sentence2_data_indices_voc3 = token2index_dataset(val_sentence2_tokens,token2id_combined_sent_voc3)\n",
    "\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_data_indices)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_data_indices)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_data_indices)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'are', 'sitting', 'and', 'lying', 'on', 'steps']\n",
      "[14, 9, 30, 8, 490, 7, 402]\n",
      "['people', 'on', 'steps']\n",
      "[14, 7, 402]\n"
     ]
    }
   ],
   "source": [
    "#visualize a random sentence1 and sentence2 paired training example\n",
    "rand_training_example = random.randint(0, len(train_sentence1) - 1)\n",
    "print (train_sentence1_tokens[rand_training_example])\n",
    "print(train_sentence1_data_indices[rand_training_example])\n",
    "\n",
    "print (train_sentence2_tokens[rand_training_example])\n",
    "print(train_sentence2_data_indices[rand_training_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 average is: 12.94135, std dev is: 5.755700667816214, max is: 78, min is: 2\n",
      "sentence2 average is: 7.43372, std dev is: 3.0907033118046123, max is: 38, min is: 1\n"
     ]
    }
   ],
   "source": [
    "#Check average, max, min sentence lengths to determine word padding\n",
    "total_sent1_len=0\n",
    "total_sent2_len=0\n",
    "sent1_lens=[]\n",
    "sent2_lens=[]\n",
    "for i in range(0,len(train_sentence1_tokens)):\n",
    "    total_sent1_len+=len(train_sentence1_tokens[i])\n",
    "    total_sent2_len+=len(train_sentence2_tokens[i])\n",
    "    sent1_lens.append(len(train_sentence1_tokens[i]))\n",
    "    sent2_lens.append(len(train_sentence2_tokens[i]))\n",
    "\n",
    "avg1=total_sent1_len/len(train_sentence1)\n",
    "avg2=total_sent2_len/len(train_sentence2)\n",
    "print(\"sentence1 average is: \"+str(avg1)+\", std dev is: \"+str(np.std(sent1_lens))+\", max is: \"+str(max(sent1_lens))+\", min is: \"+str(min(sent1_lens)))\n",
    "print(\"sentence2 average is: \"+str(avg2)+\", std dev is: \"+str(np.std(sent2_lens))+\", max is: \"+str(max(sent2_lens))+\", min is: \"+str(min(sent2_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list1, data_list2, target_list, MAX_SENTENCE_LENGTH):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.MAX_SENTENCE_LENGTH=MAX_SENTENCE_LENGTH\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.data_list2[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [token_idx1, token_idx2, len(token_idx1), len(token_idx2), label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsGroupDataset(train_sentence1_data_indices,train_sentence2_data_indices, train_targets, 35)\n",
    "val_dataset = NewsGroupDataset(val_sentence1_data_indices, val_sentence2_data_indices, val_targets, 35)\n",
    "\n",
    "train_dataset_voc2 = NewsGroupDataset(train_sentence1_data_indices_voc2,train_sentence2_data_indices_voc2, train_targets, 35)\n",
    "val_dataset_voc2 = NewsGroupDataset(val_sentence1_data_indices_voc2, val_sentence2_data_indices_voc2, val_targets, 35)\n",
    "\n",
    "#train_dataset_voc3 = NewsGroupDataset(train_sentence1_data_indices_voc3,train_sentence2_data_indices_voc3, train_targets, 35)\n",
    "#val_dataset_voc3 = NewsGroupDataset(val_sentence1_data_indices_voc3, val_sentence2_data_indices_voc3, val_targets, 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH=35\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "    for datum in batch:\n",
    "        \n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2)), torch.LongTensor(length_list1), torch.LongTensor(length_list2), torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "train_loader_voc2 = torch.utils.data.DataLoader(dataset=train_dataset_voc2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader_voc2 = torch.utils.data.DataLoader(dataset=val_dataset_voc2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "#train_loader_voc3 = torch.utils.data.DataLoader(dataset=train_dataset_voc3, \n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=newsgroup_collate_func,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "#val_loader_voc3 = torch.utils.data.DataLoader(dataset=val_dataset_voc3, \n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=newsgroup_collate_func,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "#dataloader_list=[[train_loader,val_loader,vocab1+2],[train_loader_voc2,val_loader_voc2,vocab2+2],[train_loader_voc3,val_loader_voc3,vocab3+2]]\n",
    "dataloader_list=[[train_loader,val_loader,vocab1+2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,  391,   65,  ...,    0,    0,    0],\n",
      "        [   2,   41,    7,  ...,    0,    0,    0],\n",
      "        [   2,   26, 1582,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 139, 1895,   22,  ...,    0,    0,    0],\n",
      "        [   2,   12,    5,  ...,    0,    0,    0],\n",
      "        [  13,   61,   81,  ...,    0,    0,    0]])\n",
      "torch.Size([32, 35])\n",
      "tensor([[  18,  380,    4,  ...,    0,    0,    0],\n",
      "        [ 305,   14,  312,  ...,    0,    0,    0],\n",
      "        [   3,   26,    5,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 139,   22, 1492,  ...,    0,    0,    0],\n",
      "        [   2,   12,  499,  ...,    0,    0,    0],\n",
      "        [ 509,    9,  151,  ...,    0,    0,    0]])\n",
      "torch.Size([32, 35])\n",
      "tensor([14, 10,  8, 16,  7,  6,  7, 12,  8, 13, 16,  9, 17, 14, 10,  7, 15,  8,\n",
      "         5,  7, 16, 12, 20, 14, 13,  5,  6, 11,  8, 17, 22, 10])\n",
      "torch.Size([32])\n",
      "tensor([0, 2, 0, 1, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 2, 2, 2, 2, 0, 1, 1])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "#sent1 example\n",
    "for i,(data1, data2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "    print(data1)\n",
    "    print(data1.shape)\n",
    "    print(data2)\n",
    "    print(data2.shape)\n",
    "    print(lengths1)\n",
    "    print(lengths1.shape)\n",
    "    print(labels)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a Bag of Words in PyTorch -- as an `nn.Module`.\n",
    "\n",
    "A `nn.Module` can really be any function, but it is often used to implement layers, functions and models. Note that you can also nest modules.\n",
    "\n",
    "Importantly, modules need to have their `forward()` method overridden, and very often you will want to override the `__init__` method as well. \n",
    "\n",
    "The `__init__` method sets up the module. This is also often where the internal modules and parameters are initialized.\n",
    "\n",
    "The `forward` method defines what happens when you *apply* the module.\n",
    "\n",
    "In the background, PyTorch makes use of your code in the forward method and determines how to implement back-propagation with it - but all you need to do is to define the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]\n",
    "test2 = [[1,2,3,],[5,6,7],[9,10,11]]\n",
    "\n",
    "def concat_matrices(M1,M2):\n",
    "    new_matrix = []\n",
    "    for i in range(len(M1)):\n",
    "        new_matrix.append(M1[i] + M2[i])\n",
    "        \n",
    "    return new_matrix\n",
    "\n",
    "concat_matrices(test1, test2)\n",
    "\n",
    "test3 = [1,2,3,4,5]\n",
    "test4 = [5,5,5,5,5]\n",
    "list( map(add, test3, test4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, comb_method, model_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.model_type=model_type\n",
    "        self.comb_method=comb_method\n",
    "        #Want 2 hidden layers.  Dims can change out of linear1 must = input of linear2\n",
    "        if self.model_type in ['NN','LG']:\n",
    "            if self.comb_method in ['concat','sum','product']: #=='concat':\n",
    "                if self.comb_method=='concat':\n",
    "                    self.linear1 = nn.Linear(emb_dim*2,50)\n",
    "                else:\n",
    "                    self.linear1 = nn.Linear(emb_dim,50)\n",
    "                if self.model_type=='NN':\n",
    "                    self.linear2 = nn.Linear(50,25)\n",
    "                    self.linear3 = nn.Linear(25,3)\n",
    "            else:\n",
    "                raise Exception('Vect comb methods incl concat, sum, or mult. Comb used was: {}'.format(self.comb_method))\n",
    "        else:\n",
    "            raise Exception('Model types incl NN (neural network) or LG (logistic regression).  Model used was: {}'.format(self.model_type))\n",
    "    \n",
    "    def forward(self, data1, data2, lengths1, lengths2):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        data1_vecrep=self.embed(data1)\n",
    "        data2_vecrep=self.embed(data2)\n",
    "        \n",
    "        out1=torch.sum(data1_vecrep, dim=1)\n",
    "        out2=torch.sum(data2_vecrep, dim=1)\n",
    "        \n",
    "        out1 /= lengths1.view(lengths1.size()[0],1).expand_as(out1).float()\n",
    "        out2 /= lengths2.view(lengths2.size()[0],1).expand_as(out2).float()\n",
    "        \n",
    "        if self.comb_method=='concat':\n",
    "            out=torch.cat((out1,out2), dim=1, out=None)\n",
    "        elif self.comb_method=='sum':\n",
    "            out=out1+out2\n",
    "        elif self.comb_method=='product':\n",
    "            out=out1*out2\n",
    "        \n",
    "        out = self.linear1(out.float())\n",
    "        \n",
    "        if self.model_type=='NN':\n",
    "            out = F.relu(out)\n",
    "            out = self.linear2(out.float())\n",
    "            out = F.relu(out)\n",
    "            out = self.linear3(out.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our Bag of Words model we haven't applied softmax to the output of linear layer. Why?\n",
    "We use `nn.CrossEntropyLoss()` to train. From pytorch documentation for `nn.CrossEntropyLoss()` ( https://pytorch.org/docs/stable/nn.html ) - this criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. So, this is actually exactly the same as minimizing the log likelihood after applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models with various embedding dimensions for hyperparameter tuning\n",
    "model = BagOfWords(len(id2token_combined_sent), 100,'concat','NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 100])\n",
      "torch.Size([50, 200])\n",
      "torch.Size([50])\n",
      "torch.Size([25, 50])\n",
      "torch.Size([25])\n",
      "torch.Size([3, 25])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, data_batch2, length_batch1, length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs,labels)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_type, epochs, sent_comb_method, vocab_size, embed_dim, optimizer, learning_rate, regularization, train_acc, val_acc, train_loss, val_loss]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pandas DF to store results\n",
    "results_df=pd.DataFrame(columns=['model_type','epochs','sent_comb_method','vocab_size','embed_dim',\\\n",
    "\t'optimizer','learning_rate','regularization','train_acc','val_acc','train_loss','val_loss'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model combinations to run: 32\n"
     ]
    }
   ],
   "source": [
    "#loop test, use to get the total number of combinations that will run\n",
    "model_types=['NN','LG']\n",
    "comb_methods=['product','concat']\n",
    "dim_testsizes=[100,200]\n",
    "optimizer_type=['Adam']\n",
    "learning_rates=[.01]\n",
    "l2_reg=[0,.01]\n",
    "vocab_size=[10000,15000]\n",
    "dataloader_list=[[train_loader,val_loader,vocab1+2],[train_loader_voc2,val_loader_voc2,vocab2+2]]\n",
    "\n",
    "counter=0\n",
    "\n",
    "for regularization in l2_reg:\n",
    "    for learning_rate in learning_rates:\n",
    "        for opt_type in optimizer_type:\n",
    "            for dimension in dim_testsizes:\n",
    "                for combination_method in comb_methods:\n",
    "                    for model_type in model_types:\n",
    "                        for loader in dataloader_list:\n",
    "                            counter+=1\n",
    "print(\"Model combinations to run: \"+str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping counter value: 0\n",
      "now running model: 1\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 65.6\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.9\n",
      "now running model: 2\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.5\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "now running model: 3\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.4\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "now running model: 4\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 65.5\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "now running model: 5\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.3\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "now running model: 6\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 60.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 60.8\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 61.0\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 60.3\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.4\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 60.5\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 59.9\n",
      "now running model: 7\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 60.2\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 59.5\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 60.6\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 60.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 60.3\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 60.3\n",
      "now running model: 8\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.3\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "now running model: 9\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 61.4\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "now running model: 10\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.9\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 60.4\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 59.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 60.3\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 60.4\n",
      "now running model: 11\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.0\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 62.9\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "now running model: 12\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.5\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.9\n",
      "now running model: 13\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 64.1\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 63.0\n",
      "now running model: 14\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 60.2\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 60.3\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 60.1\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 59.5\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 60.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "now running model: 15\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 60.6\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.0\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 59.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 61.0\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 61.1\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 59.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 59.0\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 59.2\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 60.1\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 59.8\n",
      "now running model: 16\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 67.3\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 66.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.5\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "now running model: 17\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 67.2\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 67.1\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 65.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "now running model: 18\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 66.8\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 64.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "now running model: 19\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.5\n",
      "now running model: 20\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.7\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 68.4\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 70.0\n",
      "now running model: 21\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 69.5\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 67.1\n",
      "now running model: 22\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 61.8\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.9\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 61.8\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "now running model: 23\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 61.1\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 61.1\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "now running model: 24\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 68.5\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 69.0\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 68.1\n",
      "now running model: 25\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.0\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 67.9\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 68.2\n",
      "now running model: 26\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 65.3\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 65.2\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.1\n",
      "now running model: 27\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 67.1\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 67.6\n",
      "now running model: 28\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.4\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 67.5\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 65.6\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "now running model: 29\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 63.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 65.8\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 66.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 65.6\n",
      "now running model: 30\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 60.5\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 61.2\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 61.1\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.5\n",
      "now running model: 31\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.6\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 62.2\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 60.8\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.1\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 61.7\n"
     ]
    }
   ],
   "source": [
    "#longer dim sizes\n",
    "model_types=['NN','LG']\n",
    "comb_methods=['product','concat']\n",
    "dim_testsizes=[100,200]\n",
    "optimizer_type=['Adam']\n",
    "learning_rates=[.01]\n",
    "l2_reg=[0,.00001]\n",
    "vocab_size=[10000,15000]\n",
    "dataloader_list=[[train_loader,val_loader,vocab1+2],[train_loader_voc2,val_loader_voc2,vocab2+2]]\n",
    "\n",
    "#short list versions for faster testing\n",
    "#dataloader_list=[[train_loader,val_loader,vocab1+2]]\n",
    "#dim_testsizes=[100]\n",
    "#comb_methods=['product']\n",
    "#learning_rates = [0.01]\n",
    "#model_types=['NN']\n",
    "#optimizer_type=['Adam']\n",
    "#l2_reg=[.00001]\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#create counter for distributing the computations\n",
    "counter=0\n",
    "\n",
    "#create a list for the numbered loops we want to run\n",
    "to_run=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\n",
    "#to_run=[22] #,22,23,24,25,26,27,28,29,30,31,32]\n",
    "\n",
    "\n",
    "#search for best results\n",
    "for regularization in l2_reg:\n",
    "    for learning_rate in learning_rates:\n",
    "        for opt_type in optimizer_type:\n",
    "            for dimension in dim_testsizes:\n",
    "                for combination_method in comb_methods:\n",
    "                    for model_type in model_types:\n",
    "                        for loader in dataloader_list:\n",
    "                            if counter in to_run:\n",
    "                                print(\"now running model: \"+str(counter))\n",
    "                                model=BagOfWords(loader[2], dimension, combination_method, model_type)\n",
    "                                if opt_type=='Adam':\n",
    "                                    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                elif opt_type=='SGD':\n",
    "                                    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "                                save_str=('v2_'+model_type+'_'+combination_method+'_'+str(loader[2])+'_'+str(dimension)+'_'+str(regularization).replace('0.',''))\n",
    "                                #check to see if we have already run this model/epoch/comb method/vocab size/embed combination\n",
    "                                #if we haven't run it yet, run it and then save\n",
    "                                if not os.path.exists(save_str):\n",
    "                                    epoch_models=[]\n",
    "                                    for epoch in range(num_epochs):\n",
    "                                        for i, (data1, data2, lengths1, lengths2, labels) in enumerate(loader[0]):\n",
    "                                            model.train()\n",
    "                                            data_batch1, data_batch2, length_batch1, length_batch2, label_batch = data1, data2, lengths1, lengths2, labels\n",
    "                                            optimizer.zero_grad()\n",
    "                                            outputs = model(data_batch1, data_batch2, length_batch1, length_batch2)\n",
    "                                            loss = criterion(outputs, label_batch)\n",
    "                                            loss.backward()\n",
    "                                            optimizer.step()\n",
    "                                            # validate every x iterations\n",
    "                                            if i > 0 and i % 3124 == 0:\n",
    "                                                # validate\n",
    "                                                val_acc = test_model(loader[1], model)[0]\n",
    "                                                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                                                    epoch+1, num_epochs, i+1, len(loader[0]), val_acc))\n",
    "                                        #here we have finished the epoch, so save our results\n",
    "                                        epoch_models.append([epoch,model,test_model(loader[1], model)[0]])\n",
    "                                    #here all epochs have run, so we want to save only the model with the best val accuracy\n",
    "                                    max_acc=max([i[2] for i in epoch_models])\n",
    "                                    max_indx=[i[2] for i in epoch_models].index(max_acc)\n",
    "                                    model=epoch_models[max_indx][1]\n",
    "                                    #now save the model so we don't have to rerun\n",
    "                                    torch.save(model.state_dict(),save_str)\n",
    "                                #if we have already run this model/epoch/comb method/vocab size/embed combination, load instead\n",
    "                                else:\n",
    "                                    model.load_state_dict(torch.load(save_str))\n",
    "\n",
    "                                #now we want to save results, but only if we don't yet have these results in the table\n",
    "                                if not ((results_df['model_type'] == model_type) & (results_df['epochs'] == num_epochs) & \\\n",
    "                                (results_df['sent_comb_method']==combination_method) & \\\n",
    "                                (results_df['vocab_size']==model.embed.num_embeddings-2) & \\\n",
    "                                (results_df['embed_dim']==model.embed.embedding_dim) & \\\n",
    "                                (results_df['learning_rate']==learning_rate) & (results_df['optimizer']==opt_type) & \\\n",
    "                                (results_df['regularization']==regularization)).any():\n",
    "                                    results_df=results_df.append(pd.Series([model_type,num_epochs,combination_method,model.embed.num_embeddings-2,model.embed.embedding_dim,opt_type,learning_rate,regularization,test_model(loader[0], model)[0],test_model(loader[1], model)[0],test_model(loader[0], model)[1].item(),test_model(loader[1], model)[1].item()],index=results_df.columns),ignore_index=True)\n",
    "                            else:\n",
    "                                print(\"skipping counter value: \"+str(counter))\n",
    "                            #increment the counter once we are done\n",
    "                            counter+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our results to a CSV if we don't already have one\n",
    "if not os.path.exists('results_table'):\n",
    "    results_df.to_csv(\"results_table\")\n",
    "else:\n",
    "    results_df=pd.read_csv(\"results_table\",usecols=[1,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>sent_comb_method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>regularization</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>75.856</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.844871</td>\n",
       "      <td>0.997327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>83.245</td>\n",
       "      <td>68.2</td>\n",
       "      <td>0.728596</td>\n",
       "      <td>0.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>83.376</td>\n",
       "      <td>68.1</td>\n",
       "      <td>0.845691</td>\n",
       "      <td>0.877152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.902</td>\n",
       "      <td>67.6</td>\n",
       "      <td>3.215102</td>\n",
       "      <td>3.322199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>76.151</td>\n",
       "      <td>67.1</td>\n",
       "      <td>0.862027</td>\n",
       "      <td>0.937692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>82.543</td>\n",
       "      <td>66.9</td>\n",
       "      <td>0.816725</td>\n",
       "      <td>1.056434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.222</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.827081</td>\n",
       "      <td>1.016528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>74.578</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.852415</td>\n",
       "      <td>0.933229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>91.102</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.652156</td>\n",
       "      <td>1.009378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.752</td>\n",
       "      <td>66.5</td>\n",
       "      <td>3.334356</td>\n",
       "      <td>3.305007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.626</td>\n",
       "      <td>66.1</td>\n",
       "      <td>3.261423</td>\n",
       "      <td>3.270649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>73.900</td>\n",
       "      <td>65.6</td>\n",
       "      <td>0.894433</td>\n",
       "      <td>0.898248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>97.340</td>\n",
       "      <td>64.8</td>\n",
       "      <td>0.584040</td>\n",
       "      <td>0.655986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>81.675</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.300332</td>\n",
       "      <td>3.525836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.422</td>\n",
       "      <td>63.1</td>\n",
       "      <td>2.956391</td>\n",
       "      <td>3.323969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>88.543</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.703158</td>\n",
       "      <td>0.873891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.729</td>\n",
       "      <td>62.7</td>\n",
       "      <td>0.575442</td>\n",
       "      <td>0.682155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.295</td>\n",
       "      <td>62.6</td>\n",
       "      <td>2.954807</td>\n",
       "      <td>3.298273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66.783</td>\n",
       "      <td>62.4</td>\n",
       "      <td>3.352863</td>\n",
       "      <td>3.504542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.106</td>\n",
       "      <td>62.2</td>\n",
       "      <td>2.947797</td>\n",
       "      <td>3.384999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>66.698</td>\n",
       "      <td>62.2</td>\n",
       "      <td>3.392915</td>\n",
       "      <td>3.320192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>86.818</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.793801</td>\n",
       "      <td>1.024558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.270</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.584388</td>\n",
       "      <td>0.766349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>90.940</td>\n",
       "      <td>61.7</td>\n",
       "      <td>0.719942</td>\n",
       "      <td>0.773133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>67.357</td>\n",
       "      <td>61.7</td>\n",
       "      <td>3.388863</td>\n",
       "      <td>3.373365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN</td>\n",
       "      <td>product</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>98.963</td>\n",
       "      <td>61.5</td>\n",
       "      <td>0.570284</td>\n",
       "      <td>1.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>67.084</td>\n",
       "      <td>61.5</td>\n",
       "      <td>3.415791</td>\n",
       "      <td>3.449479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>70.297</td>\n",
       "      <td>61.2</td>\n",
       "      <td>3.399898</td>\n",
       "      <td>3.307295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LG</td>\n",
       "      <td>product</td>\n",
       "      <td>10000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.250</td>\n",
       "      <td>60.4</td>\n",
       "      <td>2.949743</td>\n",
       "      <td>3.276784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>73.753</td>\n",
       "      <td>60.3</td>\n",
       "      <td>3.257515</td>\n",
       "      <td>3.439494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>71.696</td>\n",
       "      <td>59.9</td>\n",
       "      <td>3.377287</td>\n",
       "      <td>3.432222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LG</td>\n",
       "      <td>concat</td>\n",
       "      <td>15000</td>\n",
       "      <td>200</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>72.633</td>\n",
       "      <td>59.8</td>\n",
       "      <td>3.292809</td>\n",
       "      <td>3.358293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type sent_comb_method  vocab_size  embed_dim optimizer  \\\n",
       "20         NN           concat       10000        100      Adam   \n",
       "25         NN          product       15000        200      Adam   \n",
       "24         NN          product       10000        200      Adam   \n",
       "27         LG          product       15000        200      Adam   \n",
       "21         NN           concat       15000        100      Adam   \n",
       "17         NN          product       15000        100      Adam   \n",
       "16         NN          product       10000        100      Adam   \n",
       "28         NN           concat       10000        200      Adam   \n",
       "4          NN           concat       10000        100      Adam   \n",
       "19         LG          product       15000        100      Adam   \n",
       "26         LG          product       10000        200      Adam   \n",
       "29         NN           concat       15000        200      Adam   \n",
       "0          NN          product       10000        100      Adam   \n",
       "18         LG          product       10000        100      Adam   \n",
       "11         LG          product       15000        200      Adam   \n",
       "13         NN           concat       15000        200      Adam   \n",
       "8          NN          product       10000        200      Adam   \n",
       "3          LG          product       15000        100      Adam   \n",
       "23         LG           concat       15000        100      Adam   \n",
       "2          LG          product       10000        100      Adam   \n",
       "22         LG           concat       10000        100      Adam   \n",
       "12         NN           concat       10000        200      Adam   \n",
       "1          NN          product       15000        100      Adam   \n",
       "5          NN           concat       15000        100      Adam   \n",
       "31         LG           concat       15000        200      Adam   \n",
       "9          NN          product       15000        200      Adam   \n",
       "30         LG           concat       10000        200      Adam   \n",
       "14         LG           concat       10000        200      Adam   \n",
       "10         LG          product       10000        200      Adam   \n",
       "7          LG           concat       15000        100      Adam   \n",
       "6          LG           concat       10000        100      Adam   \n",
       "15         LG           concat       15000        200      Adam   \n",
       "\n",
       "    learning_rate  regularization  train_acc  val_acc  train_loss  val_loss  \n",
       "20           0.01         0.00001     75.856     70.0    0.844871  0.997327  \n",
       "25           0.01         0.00001     83.245     68.2    0.728596  0.791601  \n",
       "24           0.01         0.00001     83.376     68.1    0.845691  0.877152  \n",
       "27           0.01         0.00001     81.902     67.6    3.215102  3.322199  \n",
       "21           0.01         0.00001     76.151     67.1    0.862027  0.937692  \n",
       "17           0.01         0.00001     82.543     66.9    0.816725  1.056434  \n",
       "16           0.01         0.00001     81.222     66.7    0.827081  1.016528  \n",
       "28           0.01         0.00001     74.578     66.7    0.852415  0.933229  \n",
       "4            0.01         0.00000     91.102     66.7    0.652156  1.009378  \n",
       "19           0.01         0.00001     81.752     66.5    3.334356  3.305007  \n",
       "26           0.01         0.00001     81.626     66.1    3.261423  3.270649  \n",
       "29           0.01         0.00001     73.900     65.6    0.894433  0.898248  \n",
       "0            0.01         0.00000     97.340     64.8    0.584040  0.655986  \n",
       "18           0.01         0.00001     81.675     64.7    3.300332  3.525836  \n",
       "11           0.01         0.00000     99.422     63.1    2.956391  3.323969  \n",
       "13           0.01         0.00000     88.543     63.0    0.703158  0.873891  \n",
       "8            0.01         0.00000     98.729     62.7    0.575442  0.682155  \n",
       "3            0.01         0.00000     99.295     62.6    2.954807  3.298273  \n",
       "23           0.01         0.00001     66.783     62.4    3.352863  3.504542  \n",
       "2            0.01         0.00000     99.106     62.2    2.947797  3.384999  \n",
       "22           0.01         0.00001     66.698     62.2    3.392915  3.320192  \n",
       "12           0.01         0.00000     86.818     61.9    0.793801  1.024558  \n",
       "1            0.01         0.00000     98.270     61.9    0.584388  0.766349  \n",
       "5            0.01         0.00000     90.940     61.7    0.719942  0.773133  \n",
       "31           0.01         0.00001     67.357     61.7    3.388863  3.373365  \n",
       "9            0.01         0.00000     98.963     61.5    0.570284  1.002020  \n",
       "30           0.01         0.00001     67.084     61.5    3.415791  3.449479  \n",
       "14           0.01         0.00000     70.297     61.2    3.399898  3.307295  \n",
       "10           0.01         0.00000     99.250     60.4    2.949743  3.276784  \n",
       "7            0.01         0.00000     73.753     60.3    3.257515  3.439494  \n",
       "6            0.01         0.00000     71.696     59.9    3.377287  3.432222  \n",
       "15           0.01         0.00000     72.633     59.8    3.292809  3.358293  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2.sort_values(by=['val_acc'],axis=0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Correctly and Incorrectly Classified Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following three instances were classified incorrectly:\n",
      "#########################################################\n",
      "sentence 1:  a sunny street with a red building parked cars and a man walking down the road \n",
      "sentence 2:  the sun is reflecting off the windows of the red building \n",
      "Real Label:  2\n",
      "Predicted Label:  0\n",
      "\n",
      "sentence 1:  several people with parachutes are overlooking a beautiful view of fields and hills \n",
      "sentence 2:  <unk> survey their flight <unk> \n",
      "Real Label:  2\n",
      "Predicted Label:  0\n",
      "\n",
      "sentence 1:  two dogs play with tennis ball in field \n",
      "sentence 2:  dogs are watching a tennis match \n",
      "Real Label:  0\n",
      "Predicted Label:  2\n",
      "\n",
      "\n",
      "\n",
      "The following three instances were classified correctly:\n",
      "#########################################################\n",
      "sentence 1:  a boy is posing next to his scooter \n",
      "sentence 2:  a boy is next to a scooter \n",
      "Real Label:  1\n",
      "Predicted Label:  1\n",
      "\n",
      "sentence 1:  a man is sitting outside wearing blue pants \n",
      "sentence 2:  the person is inside \n",
      "Real Label:  0\n",
      "Predicted Label:  0\n",
      "\n",
      "sentence 1:  a man skateboarding at a skateboarding park which is covered in graffiti \n",
      "sentence 2:  a man is rollerblading \n",
      "Real Label:  0\n",
      "Predicted Label:  0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaklevs/anaconda3/envs/choose_a_name/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "def inverse_tokenize(sent):\n",
    "    output = ''\n",
    "    for word in sent:\n",
    "        output += id2token_combined_sent[word]\n",
    "        output += ' '\n",
    "    return output\n",
    "\n",
    "def six_classified_sentences(model_save_path):\n",
    "    filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = model_save_path.split('_')\n",
    "    model=BagOfWords(int(vocab_size), int(emb_dim), combine_type, model_type)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "    \n",
    "    misclassifieds = []\n",
    "    classifieds = []\n",
    "    \n",
    "    #index = -1\n",
    "    while len(misclassifieds) < 3 or len(classifieds) < 3:\n",
    "        #index += 1\n",
    "        index = np.random.randint(len(val_dataset))\n",
    "        sent1, sent2, len1, len2, label = val_dataset[index]\n",
    "        output = F.softmax(model(torch.tensor([sent1]), torch.tensor([sent2]), torch.tensor([len1]), torch.tensor([len2])))\n",
    "        predicted = output.max(1, keepdim=True)[1]\n",
    "        if int(predicted) != label and len(misclassifieds) < 3:\n",
    "            misclassifieds.append([inverse_tokenize(sent1), inverse_tokenize(sent2), label, int(predicted), index])\n",
    "        elif int(predicted) == label and len(classifieds) < 3:\n",
    "            classifieds.append([inverse_tokenize(sent1), inverse_tokenize(sent2), label, int(predicted), index])\n",
    "    return misclassifieds, classifieds\n",
    "\n",
    "incorrect, correct = six_classified_sentences('v2_NN_concat_10002_100_1e-05')\n",
    "\n",
    "print('The following three instances were classified incorrectly:')\n",
    "print('#########################################################')\n",
    "for instance in incorrect:\n",
    "    sent1, sent2, true_label, pred_label, index = instance\n",
    "    print('sentence 1: ', sent1)\n",
    "    print('sentence 2: ', sent2)\n",
    "    print('Real Label: ', true_label)\n",
    "    print('Predicted Label: ', pred_label)\n",
    "    print('')\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print('The following three instances were classified correctly:')\n",
    "print('#########################################################')\n",
    "for instance in correct:\n",
    "    sent1, sent2, true_label, pred_label, index = instance\n",
    "    print('sentence 1: ', sent1)\n",
    "    print('sentence 2: ', sent2)\n",
    "    print('Real Label: ', true_label)\n",
    "    print('Predicted Label: ', pred_label)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_df = pd.read_csv('mnli_train.tsv', sep=\"\\t\")\n",
    "mnli_val_df = pd.read_csv('mnli_val.tsv',sep=\"\\t\")\n",
    "\n",
    "#Creates a list of genres represented in data\n",
    "all_genres = []\n",
    "for genre in mnli_train_df['genre']:\n",
    "    if not genre in all_genres:\n",
    "        all_genres.append(genre)\n",
    "\n",
    "#similar to tokenize dataset function, but tokenizes with the pasted in vocab dictionary\n",
    "def tokenize_with_vocab(dataset,vocab):\n",
    "    output = []\n",
    "    for sentence in dataset:\n",
    "        tokens = tokenize(sentence)\n",
    "        index_list = [vocab[token] if token in vocab.keys() else 1 for token in tokens]\n",
    "        output.append(index_list)\n",
    "        \n",
    "    return output\n",
    "\n",
    "#creates training datasets for each genre\n",
    "for genre in all_genres:\n",
    "    if not os.path.exists('val_sentence2_' +genre+'.p'):\n",
    "        dataset = mnli_val_df.loc[mnli_val_df['genre'] == genre]\n",
    "        sentence1_dataset = list(dataset['sentence1'])\n",
    "        sentence2_dataset = list(dataset['sentence2'])\n",
    "        val_sentence1 = tokenize_with_vocab(sentence1_dataset, token2id_combined_sent)\n",
    "        val_sentence2 = tokenize_with_vocab(sentence2_dataset, token2id_combined_sent)\n",
    "        pkl.dump(val_sentence1, open(\"val_sentence1_\" + genre +\".p\", \"wb\"))\n",
    "        pkl.dump(val_sentence2, open(\"val_sentence2_\" + genre +\".p\", \"wb\"))\n",
    "            \n",
    "val_sentence1_telephone = pkl.load(open(\"val_sentence1_telephone.p\", \"rb\"))\n",
    "val_sentence2_telephone = pkl.load(open(\"val_sentence2_telephone.p\", \"rb\"))\n",
    "val_sentence1_fiction = pkl.load(open(\"val_sentence1_fiction.p\", \"rb\"))\n",
    "val_sentence2_fiction = pkl.load(open(\"val_sentence2_fiction.p\", \"rb\"))\n",
    "val_sentence1_slate = pkl.load(open(\"val_sentence1_slate.p\", \"rb\"))\n",
    "val_sentence2_slate = pkl.load(open(\"val_sentence2_slate.p\", \"rb\"))\n",
    "val_sentence1_government = pkl.load(open(\"val_sentence1_government.p\", \"rb\"))\n",
    "val_sentence2_government = pkl.load(open(\"val_sentence2_government.p\", \"rb\"))\n",
    "val_sentence1_travel = pkl.load(open(\"val_sentence1_travel.p\", \"rb\"))\n",
    "val_sentence2_travel = pkl.load(open(\"val_sentence2_travel.p\", \"rb\"))\n",
    "\n",
    "#intializes lists for the label\n",
    "telephone_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'telephone']['label']))\n",
    "fiction_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'fiction']['label']))\n",
    "slate_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'slate']['label']))\n",
    "government_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'government']['label']))\n",
    "travel_label = list(le.transform(mnli_val_df.loc[mnli_val_df['genre'] == 'travel']['label']))\n",
    "\n",
    "\n",
    "#sets up loaders to pass into test_model function\n",
    "val_dataset_telephone = NewsGroupDataset(val_sentence1_telephone,val_sentence2_telephone, telephone_label, 35)\n",
    "val_dataset_fiction = NewsGroupDataset(val_sentence1_fiction,val_sentence2_fiction, fiction_label, 35)\n",
    "val_dataset_slate = NewsGroupDataset(val_sentence1_slate,val_sentence2_slate, slate_label, 35)\n",
    "val_dataset_government = NewsGroupDataset(val_sentence1_government,val_sentence2_government, government_label, 35)\n",
    "val_dataset_travel = NewsGroupDataset(val_sentence1_travel,val_sentence2_travel, travel_label, 35)\n",
    "\n",
    "telephone_loader = torch.utils.data.DataLoader(dataset=val_dataset_telephone, \n",
    "                                        batch_size=len(telephone_label),\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "fiction_loader = torch.utils.data.DataLoader(dataset=val_dataset_fiction, \n",
    "                                        batch_size=len(fiction_label),\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "slate_loader = torch.utils.data.DataLoader(dataset=val_dataset_slate, \n",
    "                                        batch_size=len(slate_label),\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "government_loader = torch.utils.data.DataLoader(dataset=val_dataset_government, \n",
    "                                        batch_size=len(government_label),\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)\n",
    "\n",
    "travel_loader = torch.utils.data.DataLoader(dataset=val_dataset_travel, \n",
    "                                        batch_size=len(travel_label),\n",
    "                                        collate_fn=newsgroup_collate_func,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>telephone accuracy</th>\n",
       "      <th>fiction accuracy</th>\n",
       "      <th>slate accuracy</th>\n",
       "      <th>government</th>\n",
       "      <th>travel accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LG</td>\n",
       "      <td>38.905473</td>\n",
       "      <td>40.201005</td>\n",
       "      <td>38.922156</td>\n",
       "      <td>36.909449</td>\n",
       "      <td>37.474542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>44.278607</td>\n",
       "      <td>41.809045</td>\n",
       "      <td>40.518962</td>\n",
       "      <td>37.598425</td>\n",
       "      <td>38.492872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type  telephone accuracy  fiction accuracy  slate accuracy  \\\n",
       "0         LG           38.905473         40.201005       38.922156   \n",
       "0         NN           44.278607         41.809045       40.518962   \n",
       "\n",
       "   government  travel accuracy  \n",
       "0   36.909449        37.474542  \n",
       "0   37.598425        38.492872  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def section_3point2(model_save_path):\n",
    "    filler, model_type, combine_type, vocab_size, emb_dim, l2_reg = model_save_path.split('_')\n",
    "    model=BagOfWords(int(vocab_size), int(emb_dim), combine_type, model_type)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    telephone_accuracy = test_model(telephone_loader, model)[0]\n",
    "    fiction_accuracy = test_model(fiction_loader, model)[0]\n",
    "    slate_accuracy = test_model(slate_loader, model)[0]\n",
    "    government_accuracy = test_model(government_loader, model)[0]\n",
    "    travel_accuracy = test_model(travel_loader, model)[0]\n",
    "    \n",
    "    return pd.DataFrame({'telephone accuracy':[telephone_accuracy], 'fiction accuracy': [fiction_accuracy], \n",
    "            'slate accuracy': [slate_accuracy], 'government': [government_accuracy], 'travel accuracy':[travel_accuracy]})\n",
    "\n",
    "best_LG_model_path = 'v2_LG_product_15002_200_1e-05'\n",
    "best_NN_model_path = 'v2_NN_concat_10002_100_1e-05'\n",
    "\n",
    "LG_genre_df = pd.DataFrame(section_3point2(best_LG_model_path))\n",
    "NN_genre_df = pd.DataFrame(section_3point2(best_NN_model_path))\n",
    "\n",
    "table_3point2 = pd.concat([LG_genre_df, NN_genre_df])\n",
    "table_3point2['Model Type'] = ['LG', 'NN']\n",
    "table_3point2 = table_3point2[table_3point2.columns[-1:].tolist() + table_3point2.columns[:-1].tolist()]\n",
    "table_3point2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
