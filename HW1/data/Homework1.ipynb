{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the data\n",
    "train_df = pd.read_csv('snli_train.tsv', sep=\"\\t\")\n",
    "val_df = pd.read_csv('snli_val.tsv',sep=\"\\t\")\n",
    "\n",
    "#get data & convert sentences to lists\n",
    "train_sentence1=train_df['sentence1'].values.tolist()\n",
    "train_sentence2=train_df['sentence2'].values.tolist()\n",
    "val_sentence1=val_df['sentence1'].values.tolist()\n",
    "val_sentence2=val_df['sentence2'].values.tolist()\n",
    "\n",
    "#convert the text labels to numeric\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['label'])\n",
    "train_targets=le.transform(train_df['label']).tolist()\n",
    "val_targets=le.transform(val_df['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell only once, otherwise leverage the existing saved files\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_sentence1_tokens, train_all_sentence1_tokens = tokenize_dataset(train_sentence1)\n",
    "train_sentence2_tokens, train_all_sentence2_tokens = tokenize_dataset(train_sentence2)\n",
    "pkl.dump(train_sentence1_tokens, open(\"train_sentence1_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_sentence2_tokens, open(\"train_sentence2_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_all_sentence1_tokens, open(\"train_all_sentence1_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_all_sentence2_tokens, open(\"train_all_sentence2_tokens.p\", \"wb\"))\n",
    "\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_sentence1_tokens, _ = tokenize_dataset(val_sentence1)\n",
    "val_sentence2_tokens, _ = tokenize_dataset(val_sentence2)\n",
    "pkl.dump(val_sentence1_tokens, open(\"val_sentence1_tokens.p\", \"wb\"))\n",
    "pkl.dump(val_sentence2_tokens, open(\"val_sentence2_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have previously run the previous cell, run this cell instead to load preprocessed datasets\n",
    "train_sentence1_tokens = pkl.load(open(\"train_sentence1_tokens.p\", \"rb\"))\n",
    "train_sentence2_tokens = pkl.load(open(\"train_sentence2_tokens.p\", \"rb\"))\n",
    "train_all_sentence1_tokens = pkl.load(open(\"train_all_sentence1_tokens.p\", \"rb\"))\n",
    "train_all_sentence2_tokens = pkl.load(open(\"train_all_sentence2_tokens.p\", \"rb\"))\n",
    "val_sentence1_tokens = pkl.load(open(\"val_sentence1_tokens.p\", \"rb\"))\n",
    "val_sentence2_tokens = pkl.load(open(\"val_sentence2_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n",
      "Total number of tokens in sentence1 train dataset is 1294135\n",
      "Total number of tokens in sentence2 train dataset is 743372\n",
      "Total number of *unique* tokens in sentence1 train dataset is 14131\n",
      "Total number of *unique* tokens in sentence2 train dataset is 15225\n"
     ]
    }
   ],
   "source": [
    "#print information about the token datasets\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_tokens)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_tokens)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_tokens)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in sentence1 train dataset is {}\".format(len(train_all_sentence1_tokens)))\n",
    "print (\"Total number of tokens in sentence2 train dataset is {}\".format(len(train_all_sentence2_tokens)))\n",
    "print (\"Total number of *unique* tokens in sentence1 train dataset is {}\".format(len(set(train_all_sentence1_tokens))))\n",
    "print (\"Total number of *unique* tokens in sentence2 train dataset is {}\".format(len(set(train_all_sentence2_tokens))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocabularies for sentence1 and sentence2\n",
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id_sentence1, id2token_sentence1 = build_vocab(train_all_sentence1_tokens)\n",
    "token2id_sentence2, id2token_sentence2 = build_vocab(train_all_sentence2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 584 ; token various\n",
      "Token various; token id 584\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random \n",
    "\n",
    "random_token_id = random.randint(0, len(id2token_sentence1)-1)\n",
    "random_token = id2token_sentence1[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_sentence1[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_sentence1[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentence1 dataset size is 100000\n",
      "Train sentence2 dataset size is 100000\n",
      "Val sentence1 dataset size is 1000\n",
      "Val sentence2 dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset.  After running this cell we will have converted the word tokens to indices\n",
    "def token2index_dataset(tokens_data,token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_sentence1_data_indices = token2index_dataset(train_sentence1_tokens,token2id_sentence1)\n",
    "train_sentence2_data_indices = token2index_dataset(train_sentence2_tokens,token2id_sentence2)\n",
    "val_sentence1_data_indices = token2index_dataset(val_sentence1_tokens,token2id_sentence1)\n",
    "val_sentence2_data_indices = token2index_dataset(val_sentence2_tokens,token2id_sentence2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train sentence1 dataset size is {}\".format(len(train_sentence1_data_indices)))\n",
    "print (\"Train sentence2 dataset size is {}\".format(len(train_sentence2_data_indices)))\n",
    "print (\"Val sentence1 dataset size is {}\".format(len(val_sentence1_data_indices)))\n",
    "print (\"Val sentence2 dataset size is {}\".format(len(val_sentence2_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'white', 'woman', 'carries', 'a', 'baby', 'girl', 'while', 'walking', 'on', 'grass']\n",
      "[2, 20, 11, 538, 2, 164, 29, 23, 41, 6, 95]\n",
      "['the', 'woman', 'is', 'crawling', 'on', 'the', 'ground']\n",
      "[3, 10, 4, 2112, 8, 3, 204]\n"
     ]
    }
   ],
   "source": [
    "#visualize a random sentence1 and sentence2 paired training example\n",
    "rand_training_example = random.randint(0, len(train_sentence1) - 1)\n",
    "print (train_sentence1_tokens[rand_training_example])\n",
    "print(train_sentence1_data_indices[rand_training_example])\n",
    "\n",
    "print (train_sentence2_tokens[rand_training_example])\n",
    "print(train_sentence2_data_indices[rand_training_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 average is: 12.94135, max is: 78, min is: 2\n",
      "sentence2 average is: 7.43372, max is: 38, min is: 1\n"
     ]
    }
   ],
   "source": [
    "#Check average, max, min sentence lengths to determine word padding\n",
    "total_sent1_len=0\n",
    "total_sent2_len=0\n",
    "sent1_lens=[]\n",
    "sent2_lens=[]\n",
    "for i in range(0,len(train_sentence1_tokens)):\n",
    "    total_sent1_len+=len(train_sentence1_tokens[i])\n",
    "    total_sent2_len+=len(train_sentence2_tokens[i])\n",
    "    sent1_lens.append(len(train_sentence1_tokens[i]))\n",
    "    sent2_lens.append(len(train_sentence2_tokens[i]))\n",
    "\n",
    "avg1=total_sent1_len/len(train_sentence1)\n",
    "avg2=total_sent2_len/len(train_sentence2)\n",
    "print(\"sentence1 average is: \"+str(avg1)+\", max is: \"+str(max(sent1_lens))+\", min is: \"+str(min(sent1_lens)))\n",
    "print(\"sentence2 average is: \"+str(avg2)+\", max is: \"+str(max(sent2_lens))+\", min is: \"+str(min(sent2_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_list, sent2_list, target_list, MAX_SENTENCE_LENGTH):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_list = sent1_list\n",
    "        self.sent2_list = sent2_list\n",
    "        self.MAX_SENTENCE_LENGTH=MAX_SENTENCE_LENGTH\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_list) == len(self.sent2_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_list+self.sent2_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        #token_idx1 = self.sent1_list[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        #token_idx2 = self.sent2_list[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        token_idx= self.sent1_list[key][:self.MAX_SENTENCE_LENGTH] + self.sent2_list[key][:self.MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsGroupDataset(train_sentence1_data_indices,train_sentence2_data_indices,train_targets, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "22\n",
      "18\n",
      "23\n",
      "18\n",
      "17\n",
      "17\n",
      "15\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [2, 22, 29, 3, 2, 83, 19, 27, 6, 2, 635, 1660, 2, 298, 9, 2, 33, 20, 64, 3, 907, 118, 3, 46];\n",
      "y 2\n"
     ]
    }
   ],
   "source": [
    "## example output\n",
    "\n",
    "print(\"x {};\\ny {}\".format(train_dataset[0][0], train_dataset[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH=30\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_sentence1_data_indices, val_sentence2_data_indices, val_targets, 15)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-4c547d8617cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### checking your data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-b91d6b3c4faf>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#token_idx1 = self.sent1_list[key][:self.MAX_SENTENCE_LENGTH]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#token_idx2 = self.sent2_list[key][:self.MAX_SENTENCE_LENGTH]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtoken_idx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent1_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent2_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "### checking your data loader\n",
    "\n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print(data)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
